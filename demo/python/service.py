import camelot
import fitz  # PyMuPDF
import pandas as pd
from langchain.schema import Document
from tabulate import tabulate
import os
import win32com.client

from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import NLTKTextSplitter
from langchain_community.vectorstores import Chroma
from chromadb import PersistentClient


# í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (PyMuPDF ì‚¬ìš©)
def extract_text_with_fitz(pdf_path):
    doc = fitz.open(pdf_path)
    texts = []
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text = page.get_text()
        texts.append(text.strip() if text else "")
    return texts

# í˜ì´ì§€ë³„ ğŸ“Š í‘œ ì¶”ì¶œ í•¨ìˆ˜ (Camelot ì‚¬ìš©)
def extract_tables_lattice_per_page(pdf_path):
    tables_per_page = {}
    tables = camelot.read_pdf(pdf_path, pages="1-end", flavor="lattice")
    for table in tables:
        page_num = table.page
        df = table.df.map(lambda x: x.replace('\n', ' ') if isinstance(x, str) else x)
        if page_num not in tables_per_page:
            tables_per_page[page_num] = []
        tables_per_page[page_num].append(df)
    return tables_per_page

# í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
def convert_tables_to_markdown(tables):
    markdowns = []
    for i, df in enumerate(tables):
        try:
            markdown_table = tabulate(df.values.tolist(), headers=df.iloc[0].tolist(), tablefmt="github")
            markdowns.append(f"â–¼ í‘œ {i + 1}:\n{markdown_table}")
        except Exception as e:
            markdowns.append(f"â–¼ í‘œ {i + 1}: ë³€í™˜ ì‹¤íŒ¨ ({e})")
    return "\n\n".join(markdowns)

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
def load_embeddings_model():
    """ko-sroberta-nli ê¸°ë°˜ HuggingFace ì„ë² ë”© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
    return HuggingFaceEmbeddings(
        model_name='jhgan/ko-sroberta-nli',
        model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
        encode_kwargs={'normalize_embeddings': True},
    )

# PDF 1ê°œ ì²˜ë¦¬ -> í…ìŠ¤íŠ¸ + í‘œ ì¶”ì¶œ -> ë²¡í„°í™” -> ChromaDB ì €ì¥
def process_pdf(pdf_path, embeddings_model):
    
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    texts = extract_text_with_fitz(pdf_path)
    tables_per_page = extract_tables_lattice_per_page(pdf_path)

    documents = []

    for i, text in enumerate(texts):
        page_num = i + 1
        tables = tables_per_page.get(page_num, [])
        tables_str = convert_tables_to_markdown(tables)

        combined_text = text  # âœ… ì—¬ê¸°ì„œ ì •ì˜í•´ì•¼ í•¨

        if tables_str:
            combined_text += "\n\n---\n\nğŸ“Š í˜ì´ì§€ ë‚´ í‘œ ì •ë³´:\n" + tables_str

        doc = Document(
            page_content=combined_text,
            metadata={"page": page_num, "title": file_name}
        )
        documents.append(doc)

    if not documents:
        print(f"âš ï¸ ì¶”ì¶œëœ ë¬¸ì„œ ì—†ìŒ: {pdf_path}")
        return

    splitter = NLTKTextSplitter()
    split_documents = splitter.split_documents(documents)

    if not split_documents:
        print(f"âš ï¸ ë¶„í• ëœ ë¬¸ì„œ ì—†ìŒ: {pdf_path}")
        return

    return split_documents

# ğŸ” ëª¨ë“  PDF í´ë” ì²˜ë¦¬ ë° í•˜ë‚˜ì˜ chromaDBì— ì €ì¥
def process_all_pdfs(root_dir="./data", chroma_root="./chroma_db"):
    
    embeddings_model = load_embeddings_model()

    # âœ… PersistentClient: ChromaDBë¥¼ ë””ìŠ¤í¬ì— ì§€ì†ì ìœ¼ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ í´ë¼ì´ì–¸íŠ¸
    client = PersistentClient(path=chroma_root)
    vectorstore = Chroma(
        client=client,
        embedding_function=embeddings_model,
        collection_metadata={"hnsw:space": "cosine"}
    )
    # âœ… Chroma: ë²¡í„° ì„ë² ë”©ì„ ì €ì¥/ì¡°íšŒí•  ìˆ˜ ìˆëŠ” LangChain vectorstore
    # - client: ë””ìŠ¤í¬ ê¸°ë°˜ ì €ì¥ì„ ìœ„í•œ PersistentClient ê°ì²´
    # - embedding_function: í…ìŠ¤íŠ¸ ì„ë² ë”©ì— ì‚¬ìš©í•  ëª¨ë¸
    # - collection_metadata: ê²€ìƒ‰ì— ì‚¬ìš©í•  ê±°ë¦¬ í•¨ìˆ˜(hnsw:space) ì§€ì •
    
    for dirpath, _, filenames in os.walk(root_dir):
    # dirpath: í˜„ì¬ í´ë” ê²½ë¡œ / _ : ì´ ë³€ìˆ˜ëŠ” ì‚¬ìš© ì•ˆí• êº¼ë¼ê³  ì„ ì–¸ / filenames: íŒŒì¼ ë¦¬ìŠ¤íŠ¸  
        
        for filename in filenames:
            
            ext = filename.lower().split('.')[-1]
            
            # pdfë§Œ ì²˜ë¦¬
            if ext == "pdf":
                pdf_path = os.path.join(dirpath, filename)
                
                try:
                    split_docs = process_pdf(pdf_path, embeddings_model=embeddings_model)
                    if not split_docs:
                        continue

                    # ê¸°ì¡´ ë²¡í„° ì¤‘ë³µ ì œê±°
                    existing = vectorstore.get(include=["metadatas"]) # í˜„ì¬ ë²¡í„° DBì— ì €ì¥ëœ ë¬¸ì„œë“¤ì˜ idì™€ metadata ì •ë³´ë¥¼ ê°€ì ¸ì˜´
                    
                    doc_ids_to_delete = [
                        doc_id for doc_id, meta in zip(existing["ids"], existing["metadatas"]) # ê° ë¬¸ì„œì˜ (id, metadata)ë¥¼ ì§ì§€ì–´ ìˆœíšŒí•¨
                        if meta and meta.get("title") == split_docs[0].metadata["title"] # ì§€ê¸ˆì²˜ë¦¬ ì¤‘ì¸ ë¬¸ì„œ ì œëª©
                    ]
                    # existing["ids"]: ë¬¸ì„œì˜ ê³ ìœ  ID ë¦¬ìŠ¤íŠ¸
                    
                    if doc_ids_to_delete:
                        vectorstore.delete(doc_ids_to_delete) # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ

                    # ë°±í„° ìŠ¤í† ì–´ ì €ì¥
                    vectorstore.add_documents(split_docs)
                    
                    print(f"âœ… ì €ì¥ë¨: {filename} ({len(split_docs)} ì²­í¬)")
                    
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ - {pdf_path}: {e}")
            elif ext == "hwp":
                print(f"âš ï¸ HWP íŒŒì¼ ë¬´ì‹œë¨: {filename}")

# âœ… 8. ì‹¤í–‰ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
if __name__ == "__main__":
    process_all_pdfs()