from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
import asyncio
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from langchain_chroma import Chroma
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from pydantic import BaseModel, Field
from datetime import datetime
import logging
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager 
from typing import Dict, Optional
import os
from dotenv import load_dotenv
import uvicorn

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LangChain ì²´ì¸ (ì „ì—­ ë³€ìˆ˜)
chain = None

# ìš”ì²­ ë°ì´í„° ëª¨ë¸
class ChatRequest(BaseModel):
    message: str
    croomIdx: int
    chatter: str
    ratings: str = Field(default="5")
    createdAt: datetime = Field(default_factory=datetime.now)
    userMeta: Dict[str, Optional[str]] # ì‚¬ìš©ì ë©”íƒ€ë°ì´í„° í•„ë“œ

# ğŸŒ± lifespanìœ¼ë¡œ ì´ˆê¸°í™” ë¡œì§ êµ¬í˜„
@asynccontextmanager
async def lifespan(app: FastAPI):
    global chain
    logger.info("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ - LangChain ì´ˆê¸°í™” ì¤‘...")
    try:
        chain = create_chain()
        logger.info("âœ… LangChain ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"âœ… í˜„ì¬ ì²´ì¸ íƒ€ì…: {type(chain)}")
        logger.info(f"âœ… ì²´ì¸ êµ¬ì„±: {chain}")
    except Exception as e:
        logger.error(f"âŒ LangChain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        chain = None
    yield
    logger.info("ğŸ›‘ FastAPI ì„œë²„ ì¢…ë£Œ...")


# ğŸ FastAPI ì¸ìŠ¤í„´ìŠ¤
app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (í•„ìš” ì‹œ íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/chat")
async def chat(request: ChatRequest):
    """LangChainì„ ì‚¬ìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ + ë‹µë³€ ìƒì„±"""
    global chain
    
    if chain is None:
        raise HTTPException(status_code=500, detail="LangChainì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        logger.info(f"ğŸ“¥ ìš”ì²­ ìˆ˜ì‹ : {request.message}")

        payload = {
            "question": request.message,
            "user_name": request.userMeta.get("name", ""),
            "user_years": request.userMeta.get("years", ""),
            "user_location": request.userMeta.get("location", ""),
            "user_employees": request.userMeta.get("employees", ""),
            "user_sales": request.userMeta.get("sales", "")
        }
        
        logger.info(f"ğŸ§ª invoke payload: {payload}")


        # ë¹„ë™ê¸° ì‹¤í–‰
        response = await asyncio.to_thread(chain.invoke, payload)
        
        logger.info(f"ğŸ” response type: {type(response)}", extra={"flush": True})
        logger.info(f"ğŸ” response raw: {response}", extra={"flush": True})

        # Check if 'metadata' exists in the response 
        # [ì¤„ë°”ê¿ˆ ê°€ê³µ]
        if isinstance(response, str):
            formatted_response = response.replace("\n\n", "<br><br>").replace("\n", "<br>")
        elif isinstance(response, dict):
            value = response.get("result") or response.get("output") or response.get("text")
            formatted_response = value.replace("\n\n", "<br><br>").replace("\n", "<br>") if isinstance(value, str) else str(response)
        else:
            formatted_response = str(response)
            
        return {
            "response": formatted_response,
            "croomIdx": request.croomIdx,
            "chatter": request.chatter,
            "ratings": request.ratings,
            "createdAt": request.createdAt
        }

    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def create_chain():
    """Create and return the LangChain chain with RAG, including document title."""
    try:
        # Initialize embeddings model
        embeddings_model = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-nli',
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True},
        )
        logger.info("âœ… Embeddings model initialized.")

        # Initialize ChromaDB
        logger.info("ğŸ” Loading ChromaDB...")
        vectorstore = Chroma(
            persist_directory="./chroma_db",  # ë””ìŠ¤í¬ ê¸°ë°˜ ì €ì¥ì†Œ ì‚¬ìš©
            embedding_function=embeddings_model,
            collection_metadata={"hnsw:space": "cosine"},
        )
        logger.info("âœ… ChromaDB loaded successfully.")

        # MMR Retriever ì ìš©
        retriever = vectorstore.as_retriever(
            search_type="mmr",              # MMR ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
            search_kwargs={
                "k": 10,                    # ì „ì²´ í›„ë³´ ë¬¸ì„œ ìˆ˜
                "fetch_k": 30,              # ë” ë§ì€ í›„ë³´ ì¤‘ ë‹¤ì–‘ì„± ê³ ë ¤í•´ kê°œ ì„ íƒ
                "lambda_mult": 0.7          # ìœ ì‚¬ë„ vs ë‹¤ì–‘ì„± ê· í˜• (1.0: ìœ ì‚¬ë„ë§Œ, 0.0: ë‹¤ì–‘ì„±ë§Œ)
            }
        )

        # Define prompt
        prompt = PromptTemplate.from_template(
            """
            You are an AI assistant that recommends suitable government support programs for companies.
            Use the retrieved documents and the following company information to filter and suggest the most relevant support programs.
            If none of the programs match the company's profile, say you couldn't find a suitable one.

            [Company Information]
            - Company Name: {user_name}
            - Business Years: {user_years}
            - Location: {user_location}
            - Number of Employees: {user_employees}
            - Annual Sales Range: {user_sales}

            [User Question]
            {question}

            [Retrieved Support Program Documents]
            {context}

            Answer in Korean.
            """
        )
       
        llm = ChatOpenAI(
            openai_api_key=api_key,
            temperature=0.1,
            model_name="gpt-3.5-turbo",
            request_timeout=10
        )

        logger.info("âœ… LangChain components initialized.")

        # Runnable ì²´ì¸ì˜ ì¡°í•©, | íŒŒì´í”„ ì—°ì‚°ìë¥¼ ì¨ì„œ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ ->  RunnableSequence
        # 1. Inputìœ¼ë¡œëŠ” ì „ì²´ dictì´ ë“¤ì–´ì™€, question ê°’ë§Œ êº¼ëƒ„
        # 2. retriever: query ë¬¸ìì—´ì„ ë°›ì•„ì„œ chroma ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰ -> ê²°ê³¼ëŠ” List[Document] ê°ì²´
        # 3. RunnableLambda(lambda docs: "\n\n".join([doc.page_content for doc in docs]))
        #    retrieverì˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¦¬ìŠ¤íŠ¸ docê°€ í•˜ë‚˜ë¡œ í•©ì³ì§
        
        # ìµœì¢… ì „ì²´ dict ì…ë ¥ -> "question"ë§Œ ì¶”ì¶œ (query string) -> Chromaì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        # -> ë¬¸ì„œ ë‚´ìš©ë§Œ êº¼ë‚´ì„œ í•˜ë‚˜ë¡œ í•©ì¹¨ -> PromptTemplateì— ì „ë‹¬ë  contextê°€ ë¨
        chain = (
            {
                "context": RunnableLambda(lambda x: x["question"]) 
                | retriever 
                | RunnableLambda(lambda docs: "\n\n".join([doc.page_content for doc in docs])),
                "question": RunnablePassthrough(),
                "user_name": RunnablePassthrough(),
                "user_years": RunnablePassthrough(),
                "user_location": RunnablePassthrough(),
                "user_employees": RunnablePassthrough(),
                "user_sales": RunnablePassthrough(),
            }
            | prompt
            | llm
            | StrOutputParser()
        )

        logger.info("ğŸš€ LangChain RAG chain created successfully.")

        return chain

    except Exception as e:
        logger.error(f"âŒ Error creating LangChain chain with RAG: {e}")
        raise RuntimeError("Failed to create LangChain chain with RAG")


if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8002, reload=True)