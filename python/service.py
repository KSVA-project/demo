# google-vision OCR
# splitter: RecursiveCharacterTextSplitter : ë¬¸ë‹¨-> ì¤„ -> ë¬¸ì¥ -> ë‹¨ì–´ ì¬ê·€ì ìœ¼ë¡œ ë‚˜ëˆ”
# ì„ë² ë”© ëª¨ë¸ : ko-sroberta-nli ê¸°ë°˜ HuggingFace
# vectorstor : chromadb, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©

import fitz  # PyMuPDF
import pandas as pd
from langchain.schema import Document
from tabulate import tabulate
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from chromadb import PersistentClient
from dotenv import load_dotenv

import io
from pdf2image import convert_from_path
from google.cloud import vision
import hashlib

# âœ… ê¸°ì—…ë§ˆë‹¹ API ì •ë³´
from api_data import get_bizinfo_data_by_hashtags, filter_matched_bizinfo

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (OpenAI í‚¤ ë“±)
load_dotenv()

gcp_key = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# Google Cloud Vision í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = vision.ImageAnnotatorClient()

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
def load_embeddings_model():
    """ko-sroberta-nli ê¸°ë°˜ HuggingFace ì„ë² ë”© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
    return HuggingFaceEmbeddings(
        model_name='jhgan/ko-sroberta-nli',
        model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
        encode_kwargs={'normalize_embeddings': True},
    )

# âœ… txt í•´ì‹œ ìƒì„± í•¨ìˆ˜, ê³„ì‚°
def get_pdf_hash(pdf_path):
    with open(pdf_path, "rb") as f:
        return hashlib.sha256(f.read()).hexdigest()

# âœ… OCR ìºì‹œ í´ë”
CACHE_DIR = "./ocr_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

# âœ… Google Vision OCRë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ, txtë¡œ ì €ì¥
def extract_text_from_pdf(pdf_path, existing_hashes=None):
    
    file_hash = get_pdf_hash(pdf_path)
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    cache_path = os.path.join(CACHE_DIR, f"{file_name}.txt")
    
    # 1. ìºì‹œëœ OCR ê²°ê³¼ ìˆìœ¼ë©´ ë°”ë¡œ ë¡œë“œ
    if os.path.exists(cache_path):
        print(f"â© OCR ìºì‹œ ë¶ˆëŸ¬ì˜´: {file_name}")
        with open(cache_path, "r", encoding="utf-8") as f:
            text_by_page = f.read().split("\f")
        return text_by_page, file_hash, file_name
    
    print(f"ğŸ” OCR ìˆ˜í–‰: {pdf_path}")
    text_by_page = []
    doc = fitz.open(pdf_path)
    total_pages = len(doc)
    
    for i in range(total_pages):
        
        # [1] PDF í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        images = convert_from_path(pdf_path, dpi=300, first_page=i+1, last_page=i+1)
        image = images[0]
        
        # [2] ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ë¡œ ì €ì¥ (íŒŒì¼ ì €ì¥ ì—†ì´)
        img_byte_arr = io.BytesIO() # ë³€í™˜ëœ PIL ì´ë¯¸ì§€ ê°ì²´ë¥¼ ë©”ëª¨ë¦¬ ë‚´ ë°”ì´íŠ¸ ë°°ì—´(BytesIO)ë¡œ ì €ì¥
        image.save(img_byte_arr, format='PNG') # íŒŒì¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥í•˜ì§€ ì•Šê³  ë°”ë¡œ ë©”ëª¨ë¦¬ì—ì„œ Vision APIì— ë„˜ê¸°ê¸° ìœ„í•œ ì¤€ë¹„
        img_byte_arr = img_byte_arr.getvalue() # PNG í¬ë§·ìœ¼ë¡œ ì €ì¥ í›„ getvalue()ë¡œ ì‹¤ì œ ë°”ì´íŠ¸ ë°ì´í„° ì¶”ì¶œ
    
        # [3] Vision APIë¡œ OCR ìš”ì²­
        image_for_vision = vision.Image(content=img_byte_arr)
        response = client.text_detection(image=image_for_vision)
        texts = response.text_annotations
        
        if texts:
            text_by_page.append(texts[0].description.strip())
        else:
            text_by_page.append("")

    # OCR ê²°ê³¼ ì €ì¥
    with open(cache_path, "w", encoding="utf-8") as f:
        f.write("\f".join(text_by_page)) 
        # í¼ í”¼ë“œ (form feed) ë¬¸ì, í˜ì´ì§€ êµ¬ë¶„ìë¡œ ì‚¬ìš©
        # text_by_page == ê° í˜ì´ì§€ì˜ OCR ê²°ê³¼ í…ìŠ¤íŠ¸ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸
        
    return text_by_page, file_hash, file_name

# OCRëœ text_by_pageë¥¼ split documentsë¡œ ë°˜í™˜
def split_text_to_documents(text_by_page, file_hash, file_name, matched_df=None):
      
    documents = []
    meta_row = None # í•´ë‹¹ PDF íŒŒì¼ëª…ì— í•´ë‹¹í•˜ëŠ” ë©”íƒ€ë°ì´í„° í–‰ ì¶”ì¶œ
    
    if matched_df is not None:
        match = matched_df[matched_df["pblancNm"]== file_name]
        if not match.empty:
            meta_row = match.iloc[0].to_dict()

    for i, text in enumerate(text_by_page):
        page_num = i + 1
        metadata = {"page": page_num, 
                    "pblancNm": file_name, 
                    "file_hash": file_hash}
        if meta_row:
            metadata.update(meta_row)

        doc = Document(
            page_content=text,
            metadata=metadata
        )
        documents.append(doc)

    if not documents:
        print(f"âš ï¸ ì¶”ì¶œëœ ë¬¸ì„œ ì—†ìŒ: {file_name}")
        return

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,     # í•œ ì¡°ê° ìµœëŒ€ í† í° ìˆ˜
        chunk_overlap=150,   # ì•ë’¤ë¡œ ê²¹ì¹˜ëŠ” í† í° ìˆ˜ (ë¬¸ë§¥ ìœ ì§€ìš©)
        separators=["\n\n", "\n", ".", " ", ""],  # ë¬¸ë‹¨ â†’ ì¤„ â†’ ë¬¸ì¥ â†’ ë‹¨ì–´ â†’ ë¬¸ì
        )
    
    split_documents = splitter.split_documents(documents)

    if not split_documents:
        print(f"âš ï¸ ë¶„í• ëœ ë¬¸ì„œ ì—†ìŒ: {file_name}")
        return

    return split_documents, file_hash

# ğŸ” ëª¨ë“  PDF í´ë” ì²˜ë¦¬ ë° í•˜ë‚˜ì˜ chromaDBì— ì €ì¥
def process_cached_txt_files(root_dir="./data", chroma_root="./chroma_db"):
    
    embeddings_model = load_embeddings_model()

    # ê¸°ì—…ë§ˆë‹¹ ë©”íƒ€ë°ì´í„° matched_df ë¶ˆëŸ¬ì˜¤ê¸°
    api_data = get_bizinfo_data_by_hashtags()
    matched_df = filter_matched_bizinfo(api_data, folder_path=root_dir)

    # ChromaDB ì´ˆê¸°í™”
    # PersistentClient: ChromaDBë¥¼ ë””ìŠ¤í¬ì— ì§€ì†ì ìœ¼ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ í´ë¼ì´ì–¸íŠ¸
    client = PersistentClient(path=chroma_root)
    vectorstore = Chroma(
        persist_directory="./chroma_db", # ChromaDB ë²¡í„° ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì˜êµ¬ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        embedding_function=embeddings_model,
        collection_metadata={
            "hnsw:space": "cosine",             # ìì—°ì–´ ìœ ì‚¬ë„ ì¸¡ì •ì— ê°€ì¥ ì í•©
            "hnsw:construction_ef": 400,        # ì¸ë±ìŠ¤ í’ˆì§ˆ â†‘ (ê¸°ë³¸ 200 â†’ 400)
            "hnsw:M": 32,                       # ì—°ê²° ìˆ˜ ì¦ê°€ â†’ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ (ê¸°ë³¸ 16)
            "hnsw:search_ef": 128,              # ê²€ìƒ‰ ì‹œ ì •í™•ë„ ì¦ê°€ (ê¸°ë³¸ 10~100)
            "hnsw:num_threads": 4,              # ë©€í‹°ìŠ¤ë ˆë“œ ì¸ë±ì‹± (CPU ì„±ëŠ¥ì— ë§ê²Œ ì¡°ì ˆ)
            "hnsw:batch_size": 64               # ë²¡í„° ì‚½ì… ì†ë„ í–¥ìƒ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ)
        }
    )
    
    # âœ… Chroma: ë²¡í„° ì„ë² ë”©ì„ ì €ì¥/ì¡°íšŒí•  ìˆ˜ ìˆëŠ” LangChain vectorstore
    # - client: ë””ìŠ¤í¬ ê¸°ë°˜ ì €ì¥ì„ ìœ„í•œ PersistentClient ê°ì²´
    # - embedding_function: í…ìŠ¤íŠ¸ ì„ë² ë”©ì— ì‚¬ìš©í•  ëª¨ë¸
    # - collection_metadata: ê²€ìƒ‰ì— ì‚¬ìš©í•  ê±°ë¦¬ í•¨ìˆ˜(hnsw:space) ì§€ì •
    
    # ê¸°ì¡´ ë²¡í„° ì¤‘ë³µ ì œê±°
    # ê¸°ì¡´ì— ì €ì¥ëœ í•´ì‹œ ë¡œë”©
    existing = vectorstore.get(include=["metadatas"])
    existing_hashes = {
        meta["file_hash"]
        for meta in existing["metadatas"]
        if meta and "file_hash" in meta
    }
    # if meta and "file_hash" in meta ->  metaê°€ Noneì´ ì•„ë‹ˆê³ , "file_hash"ë¼ëŠ” í‚¤ë¥¼ í‡í•¨í•˜ê³  ìˆì„ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    
    for dirpath, _, filenames in os.walk(root_dir):
    # dirpath: í˜„ì¬ í´ë” ê²½ë¡œ / _ : ì´ ë³€ìˆ˜ëŠ” ì‚¬ìš© ì•ˆí• êº¼ë¼ê³  ì„ ì–¸ / filenames: íŒŒì¼ ë¦¬ìŠ¤íŠ¸  
        
        for filename in filenames:
            
            # PDF ë§Œ ì²˜ë¦¬
            if not filename.lower().endswith(".pdf"):
                continue
            
            pdf_path = os.path.join(dirpath, filename)
            
            try:
                # Step 1: OCR + ìºì‹œ ë¡œë”© ë˜ëŠ” ìˆ˜í–‰
                text_by_page, file_hash, file_name = extract_text_from_pdf(pdf_path, existing_hashes)
                
                # ë²¡í„°í™”ëœ íŒŒì¼ì´ë©´ ê±´ë„ˆëœ€
                if file_hash in existing_hashes:
                    print(f"â© ì´ë¯¸ ë²¡í„°í™” ì™„ë£Œëœ íŒŒì¼ì…ë‹ˆë‹¤. ê±´ë„ˆëœ€: {file_name}")
                    continue
                
                if text_by_page is None:
                    continue  # ì¤‘ë³µëœ ê²½ìš°
                
                # Step 2: split documents
                split_docs, _ = split_text_to_documents(
                    text_by_page=text_by_page,
                    file_hash=file_hash,
                    file_name=file_name,
                    matched_df=matched_df
                )
                
                if not split_docs:
                    continue
                
                # Step 3: ë²¡í„° DB ì €ì¥
                vectorstore.add_documents(split_docs)
                print(f"âœ… ì €ì¥ë¨: {filename} ({len(split_docs)} ì²­í¬)")
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ - {pdf_path}: {e}")

### ë©í¼ í•¨ìˆ˜
def run_service():
    """FastAPI ë“±ì—ì„œ import í•´ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ í•¨ìˆ˜"""
    process_cached_txt_files()


# âœ… 8. ì‹¤í–‰ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
if __name__ == "__main__":
    process_cached_txt_files()
    
