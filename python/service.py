# google-vision OCR
# splitter: RecursiveCharacterTextSplitter : ë¬¸ë‹¨-> ì¤„ -> ë¬¸ì¥ -> ë‹¨ì–´ ì¬ê·€ì ìœ¼ë¡œ ë‚˜ëˆ”
# ì„ë² ë”© ëª¨ë¸ : ko-sroberta-nli ê¸°ë°˜ HuggingFace
# vectorstor : chromadb, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©

import fitz  # PyMuPDF
import pandas as pd
from langchain.schema import Document
from tabulate import tabulate
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from chromadb import PersistentClient
from dotenv import load_dotenv
from collections import defaultdict 
import io
from pdf2image import convert_from_path
from google.cloud import vision
import hashlib

# âœ… ê¸°ì—…ë§ˆë‹¹ API ì •ë³´
from api_data import get_bizinfo_data_by_hashtags, filter_matched_bizinfo

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (OpenAI í‚¤ ë“±)
load_dotenv()

gcp_key = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# Google Cloud Vision í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = vision.ImageAnnotatorClient()

# device í™•ì¸
import torch
def get_device():
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  ì ì ˆí•œ device ë°˜í™˜"""
    if torch.cuda.is_available():
        device = 'cuda'
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name} ({gpu_count}ê°œ)")
    else:
        device = 'cpu'
        print("âš ï¸ GPU ë¶ˆê°€ëŠ¥, CPU ì‚¬ìš©")


# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
def load_embeddings_model():
    """ko-sroberta-nli ê¸°ë°˜ HuggingFace ì„ë² ë”© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
    device = get_device()
    return HuggingFaceEmbeddings(
        model_name='jhgan/ko-sroberta-nli',
        model_kwargs={'device': device},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
        encode_kwargs={'normalize_embeddings': True},
    )

# âœ… txt í•´ì‹œ ìƒì„± í•¨ìˆ˜, ê³„ì‚°
def get_pdf_hash(pdf_path):
    with open(pdf_path, "rb") as f:
        return hashlib.sha256(f.read()).hexdigest()

# âœ… OCR ìºì‹œ í´ë”
CACHE_DIR = "./ocr_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

# âœ… Google Vision OCRë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ, txtë¡œ ì €ì¥
def extract_text_from_pdf(pdf_path, existing_hashes=None):
    
    file_hash = get_pdf_hash(pdf_path)
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    cache_path = os.path.join(CACHE_DIR, f"{file_name}.txt")
    
    # 1. ìºì‹œëœ OCR ê²°ê³¼ ìˆìœ¼ë©´ ë°”ë¡œ ë¡œë“œ
    if os.path.exists(cache_path):
        print(f"â© OCR ìºì‹œ ë¶ˆëŸ¬ì˜´: {file_name}")
        with open(cache_path, "r", encoding="utf-8") as f:
            text_by_page = f.read().split("\f")
        return text_by_page, file_hash, file_name
    
    print(f"ğŸ” OCR ìˆ˜í–‰: {pdf_path}")
    text_by_page = []
    doc = fitz.open(pdf_path)
    total_pages = len(doc)
    
    for i in range(total_pages):
        
        # [1] PDF í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        images = convert_from_path(pdf_path, dpi=300, first_page=i+1, last_page=i+1)
        image = images[0]
        
        # [2] ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ë¡œ ì €ì¥ (íŒŒì¼ ì €ì¥ ì—†ì´)
        img_byte_arr = io.BytesIO() # ë³€í™˜ëœ PIL ì´ë¯¸ì§€ ê°ì²´ë¥¼ ë©”ëª¨ë¦¬ ë‚´ ë°”ì´íŠ¸ ë°°ì—´(BytesIO)ë¡œ ì €ì¥
        image.save(img_byte_arr, format='PNG') # íŒŒì¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥í•˜ì§€ ì•Šê³  ë°”ë¡œ ë©”ëª¨ë¦¬ì—ì„œ Vision APIì— ë„˜ê¸°ê¸° ìœ„í•œ ì¤€ë¹„
        img_byte_arr = img_byte_arr.getvalue() # PNG í¬ë§·ìœ¼ë¡œ ì €ì¥ í›„ getvalue()ë¡œ ì‹¤ì œ ë°”ì´íŠ¸ ë°ì´í„° ì¶”ì¶œ
    
        # [3] Vision APIë¡œ OCR ìš”ì²­
        image_for_vision = vision.Image(content=img_byte_arr)
        response = client.text_detection(image=image_for_vision)
        texts = response.text_annotations
        
        if texts:
            text_by_page.append(texts[0].description.strip())
        else:
            text_by_page.append("")

    # OCR ê²°ê³¼ ì €ì¥
    with open(cache_path, "w", encoding="utf-8") as f:
        f.write("\f".join(text_by_page)) 
        # í¼ í”¼ë“œ (form feed) ë¬¸ì, í˜ì´ì§€ êµ¬ë¶„ìë¡œ ì‚¬ìš©
        # text_by_page == ê° í˜ì´ì§€ì˜ OCR ê²°ê³¼ í…ìŠ¤íŠ¸ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸
        
    return text_by_page, file_hash, file_name

# OCRëœ text_by_pageë¥¼ split documentsë¡œ ë°˜í™˜
def split_text_to_documents(text_by_page, file_hash, file_name, matched_df=None):
    """
    OCRëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œë¡œ ë¶„í• í•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        text_by_page: í˜ì´ì§€ë³„ OCR í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        file_hash: íŒŒì¼ í•´ì‹œê°’
        file_name: í™•ì¥ì ì œê±°ëœ íŒŒì¼ëª…
        matched_df: APIì—ì„œ ê°€ì ¸ì˜¨ ë§¤ì¹­ëœ ë©”íƒ€ë°ì´í„° DataFrame
        
    Returns:
        tuple: (split_documents, file_hash) ë˜ëŠ” (None, None)
    """
    
    documents = []
    meta_row = None # í•´ë‹¹ PDF íŒŒì¼ëª…ì— í•´ë‹¹í•˜ëŠ” ë©”íƒ€ë°ì´í„° í–‰ ì¶”ì¶œ
    
    # í•´ë‹¹ íŒŒì¼ëª…ì— ë§ëŠ” ë©”íƒ€ë°ì´í„° ì°¾ê¸°
    if matched_df is not None and not matched_df.empty:
        # ì •í™•í•œ ë§¤ì¹­ í™•ì¸
        match = matched_df[matched_df["pblancNm"] == file_name]
        
        if not match.empty:
            meta_row = match.iloc[0].to_dict()
            print(f"âœ”ï¸ ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ì„±ê³µ: {file_name}")
        else:
            print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ì‹¤íŒ¨: {file_name}")
            # ë””ë²„ê¹…ì„ ìœ„í•´ ì‚¬ìš© ê°€ëŠ¥í•œ ê³µê³ ëª…ë“¤ ì¶œë ¥
            available_names = matched_df["pblancNm"].tolist()
            print(f"[DEBUG] ì‚¬ìš© ê°€ëŠ¥í•œ ê³µê³ ëª…: {available_names[:5]}")

    # ê° í˜ì´ì§€ë³„ë¡œ ë¬¸ì„œ ìƒì„±
    for i, text in enumerate(text_by_page):
        
        # ë¹ˆ í˜ì´ì§€ëŠ” ê±´ë„ˆë›°ê¸°
        if not text.strip():
            continue
        
        page_num = i + 1
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
        metadata = {
            "page": page_num, 
            "pblancNm": file_name, 
            "file_hash": file_hash
        }
        
        # API ë©”íƒ€ë°ì´í„° ì¶”ê°€
        if meta_row:
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë¥¼ ë¨¼ì € ì„¤ì •í•˜ê³ , API ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
            metadata.update(meta_row)
            # pblancNmì€ íŒŒì¼ëª…ìœ¼ë¡œ ìœ ì§€ (API ë°ì´í„°ë¡œ ë®ì–´ì“°ì§€ ì•ŠìŒ)
            metadata["pblancNm"] = file_name

        doc = Document(
            page_content=text,
            metadata=metadata
        )
        documents.append(doc)

    if not documents:
        print(f"âš ï¸ ì¶”ì¶œëœ ë¬¸ì„œ ì—†ìŒ: {file_name}")
        return

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,     # í•œ ì¡°ê° ìµœëŒ€ í† í° ìˆ˜
        chunk_overlap=150,   # ì•ë’¤ë¡œ ê²¹ì¹˜ëŠ” í† í° ìˆ˜ (ë¬¸ë§¥ ìœ ì§€ìš©)
        separators=["\n\n", "\n", ".", " ", ""],  # ë¬¸ë‹¨ â†’ ì¤„ â†’ ë¬¸ì¥ â†’ ë‹¨ì–´ â†’ ë¬¸ì
        )
    
    split_documents = splitter.split_documents(documents)

    if not split_documents:
        print(f"âš ï¸ ë¶„í• ëœ ë¬¸ì„œ ì—†ìŒ: {file_name}")
        return

    return split_documents, file_hash

# ğŸ” ëª¨ë“  PDF í´ë” ì²˜ë¦¬ ë° í•˜ë‚˜ì˜ chromaDBì— ì €ì¥
def process_cached_txt_files(root_dir="./data", chroma_root="./chroma_db"):
    
    embeddings_model = load_embeddings_model()

    # ê¸°ì—…ë§ˆë‹¹ ë©”íƒ€ë°ì´í„° matched_df ë¶ˆëŸ¬ì˜¤ê¸°
    api_data = get_bizinfo_data_by_hashtags()
    
    if not api_data:
        print("âš ï¸ API ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë©”íƒ€ë°ì´í„° ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
        matched_df = None
    else:
        print("ğŸ” ë¡œì»¬ íŒŒì¼ê³¼ ë§¤ì¹­ ì¤‘...")
        matched_df = filter_matched_bizinfo(api_data, folder_path=root_dir)
        
        if matched_df.empty:
            print("âš ï¸ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë©”íƒ€ë°ì´í„° ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
            matched_df = None
        else:
            print(f"âœ”ï¸ {len(matched_df)}ê°œ íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # ChromaDB ì´ˆê¸°í™”
    # PersistentClient: ChromaDBë¥¼ ë””ìŠ¤í¬ì— ì§€ì†ì ìœ¼ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ í´ë¼ì´ì–¸íŠ¸
    client = PersistentClient(path=chroma_root)
    vectorstore = Chroma(
        persist_directory="./chroma_db", # ChromaDB ë²¡í„° ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì˜êµ¬ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        embedding_function=embeddings_model,
        collection_metadata={
            "hnsw:space": "cosine",             # ìì—°ì–´ ìœ ì‚¬ë„ ì¸¡ì •ì— ê°€ì¥ ì í•©
            "hnsw:construction_ef": 400,        # ì¸ë±ìŠ¤ í’ˆì§ˆ â†‘ (ê¸°ë³¸ 200 â†’ 400)
            "hnsw:M": 32,                       # ì—°ê²° ìˆ˜ ì¦ê°€ â†’ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ (ê¸°ë³¸ 16)
            "hnsw:search_ef": 128,              # ê²€ìƒ‰ ì‹œ ì •í™•ë„ ì¦ê°€ (ê¸°ë³¸ 10~100)
            "hnsw:num_threads": 4,              # ë©€í‹°ìŠ¤ë ˆë“œ ì¸ë±ì‹± (CPU ì„±ëŠ¥ì— ë§ê²Œ ì¡°ì ˆ)
            "hnsw:batch_size": 64               # ë²¡í„° ì‚½ì… ì†ë„ í–¥ìƒ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ)
        }
    )
    
    # âœ… Chroma: ë²¡í„° ì„ë² ë”©ì„ ì €ì¥/ì¡°íšŒí•  ìˆ˜ ìˆëŠ” LangChain vectorstore
    # - client: ë””ìŠ¤í¬ ê¸°ë°˜ ì €ì¥ì„ ìœ„í•œ PersistentClient ê°ì²´
    # - embedding_function: í…ìŠ¤íŠ¸ ì„ë² ë”©ì— ì‚¬ìš©í•  ëª¨ë¸
    # - collection_metadata: ê²€ìƒ‰ì— ì‚¬ìš©í•  ê±°ë¦¬ í•¨ìˆ˜(hnsw:space) ì§€ì •
    
    # ê¸°ì¡´ ë²¡í„° ì¤‘ë³µ ì œê±°
    # ê¸°ì¡´ì— ì €ì¥ëœ í•´ì‹œ ë¡œë”©
    existing = vectorstore.get(include=["metadatas"])
    existing_hashes = {
        meta["file_hash"]
        for meta in existing["metadatas"]
        if meta and "file_hash" in meta
    }
    # if meta and "file_hash" in meta ->  metaê°€ Noneì´ ì•„ë‹ˆê³ , "file_hash"ë¼ëŠ” í‚¤ë¥¼ í‡í•¨í•˜ê³  ìˆì„ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    
    all_documents = []
    
    for dirpath, _, filenames in os.walk(root_dir):
    # dirpath: í˜„ì¬ í´ë” ê²½ë¡œ / _ : ì´ ë³€ìˆ˜ëŠ” ì‚¬ìš© ì•ˆí• êº¼ë¼ê³  ì„ ì–¸ / filenames: íŒŒì¼ ë¦¬ìŠ¤íŠ¸  
        
        for filename in filenames:
            
            # PDF ë§Œ ì²˜ë¦¬
            if not filename.lower().endswith(".pdf"):
                continue
            
            pdf_path = os.path.join(dirpath, filename)
            
            try:
                # Step 1: OCR + ìºì‹œ ë¡œë”© ë˜ëŠ” ìˆ˜í–‰
                text_by_page, file_hash, file_name = extract_text_from_pdf(pdf_path, existing_hashes)
                
                # ë²¡í„°í™”ëœ íŒŒì¼ì´ë©´ ê±´ë„ˆëœ€
                if file_hash in existing_hashes:
                    print(f"â© ì´ë¯¸ ë²¡í„°í™” ì™„ë£Œëœ íŒŒì¼ì…ë‹ˆë‹¤. ê±´ë„ˆëœ€: {file_name}")
                    continue
                
                if text_by_page is None:
                    continue  # ì¤‘ë³µëœ ê²½ìš°
                
                # Step 2: split documents
                split_docs, _ = split_text_to_documents(
                    text_by_page=text_by_page,
                    file_hash=file_hash,
                    file_name=file_name,
                    matched_df=matched_df
                )
                
                if split_docs:
                    all_documents.extend(split_docs)  # âœ… ëª¨ë“  ë¬¸ì„œ ìˆ˜ì§‘
                    print(f"ğŸ“„ OCR ì²˜ë¦¬ë¨: {filename} ({len(split_docs)} ì²­í¬)")
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ - {pdf_path}: {e}")
                
    print(f"ğŸ“¦ ì „ì²´ OCR ì²˜ë¦¬ëœ ë¬¸ì„œ: {len(all_documents)}ê°œ ì²­í¬")
    documents_to_save = all_documents  # ëª¨ë“  ë¬¸ì„œ ì €ì¥
    
    # âœ… ê·¸ë£¹í™” ë° ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
    
    doc_groups = defaultdict(list)
    for doc in documents_to_save:
        title = doc.metadata.get("pblancNm")
        doc_groups[title].append(doc)

    added_count = 0
    for title, doc_list in doc_groups.items():
        file_hash = doc_list[0].metadata.get("file_hash")
        if file_hash in existing_hashes:
            print(f"â© ì´ë¯¸ ì €ì¥ëœ íŒŒì¼: {title} (hash: {file_hash}) â†’ ìŠ¤í‚µë¨")
            continue

        vectorstore.add_documents(doc_list)
        added_count += 1
        print(f"âœ… ë²¡í„°DB ì €ì¥ë¨: {title} ({len(doc_list)} ì²­í¬)")

    print(f"\nğŸ“¦ ìµœì¢… ì €ì¥ ì™„ë£Œ: {added_count}ê°œ ë¬¸ì„œ (ì¤‘ë³µ ì œì™¸)")
    print(f"ğŸ¯ ì „ì²´ ì²˜ë¦¬: {len(all_documents)}ê°œ ì²­í¬")
    print(f"ğŸ¯ ë²¡í„°DB ì €ì¥: {len(documents_to_save)}ê°œ ì²­í¬")

### ë©í¼ í•¨ìˆ˜
def run_service():
    """FastAPI ë“±ì—ì„œ import í•´ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ í•¨ìˆ˜"""
    process_cached_txt_files()


# âœ… 8. ì‹¤í–‰ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
if __name__ == "__main__":
    process_cached_txt_files()
    
