# MMR Retriever ì ìš©
# ì„ë² ë”© ëª¨ë¸ : OpenAIEmbeddings

# âœ… FastAPI ê´€ë ¨
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from contextlib import asynccontextmanager
import uvicorn

# âœ… LangChain - í•µì‹¬ ì²´ì¸ êµ¬ì„± ìš”ì†Œ
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# âœ… LangChain - ëª¨ë¸ ë° ë²¡í„°ìŠ¤í† ì–´
from langchain_community.chat_models import ChatOpenAI
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# âœ… Python ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime
from typing import Dict, Optional, List
import asyncio
import logging
import os
import json

# âœ… APScheduler ì¶”ê°€
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

# âœ… í¬ë¡¤ë§ ë° ë²¡í„° DB ì €ì¥ - ì‘ì—… ìŠ¤ì¼€ì¤„ë§
from crawling import run_crawling
from service import run_service

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LangChain ì²´ì¸ (ì „ì—­ ë³€ìˆ˜)
chain = None
classifier_chain = None

# ìš”ì²­ ë°ì´í„° ëª¨ë¸
class ChatRequest(BaseModel):
    message: str
    croomIdx: int
    chatter: str
    ratings: str = Field(default="5")
    createdAt: datetime = Field(default_factory=datetime.now)
    userMeta: Dict[str, Optional[str]] # ì‚¬ìš©ì ë©”íƒ€ë°ì´í„° í•„ë“œ
    userTypes: List[str] = Field(default=[]) # ì¶”ê°€: ê¸°ì—… ìœ í˜• ë¦¬ìŠ¤íŠ¸


def scheduled_job():
    logger.info("ğŸ•’ APScheduler: run_crawling + run_service ì‹¤í–‰ ì‹œì‘")
    try:
        run_crawling()
        run_service()
        logger.info("âœ… APScheduler: ì‘ì—… ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ APScheduler: ì‘ì—… ì‹¤íŒ¨ - {e}")

# ğŸŒ± lifespanìœ¼ë¡œ ì´ˆê¸°í™” ë¡œì§ êµ¬í˜„
@asynccontextmanager
async def lifespan(app: FastAPI):
    global chain
    global classifier_chain
    logger.info("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ - LangChain ì´ˆê¸°í™” ì¤‘...")
    
    try:
        chain = create_chain()
        logger.info("âœ… LangChain ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"âœ… í˜„ì¬ ì²´ì¸ íƒ€ì…: {type(chain)}")
        logger.info(f"âœ… ì²´ì¸ êµ¬ì„±: {chain}")
        classifier_chain = classifier_chain()
        logger.info("âœ… classifier_chain ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"âœ… í˜„ì¬ ì²´ì¸ íƒ€ì…: {type(classifier_chain)}")
        
    except Exception as e:
        logger.error(f"âŒ LangChain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        chain = None
    
    # âœ… APScheduler ì‘ì—… ì‹œì‘
    scheduler = AsyncIOScheduler()
    scheduler.add_job(
        scheduled_job, 
        CronTrigger(hour=20, minute=20, timezone="Asia/Seoul")
    )
    scheduler.start()
    logger.info("ğŸ”„ APScheduler ì‹œì‘ - ë§¤ì¼ 20:20 ì‹¤í–‰")
    
    yield
    logger.info("ğŸ›‘ FastAPI ì„œë²„ ì¢…ë£Œ...")

# ğŸ FastAPI ì¸ìŠ¤í„´ìŠ¤
app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (í•„ìš” ì‹œ íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/chat")
async def chat(request: ChatRequest):
    """LangChainì„ ì‚¬ìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ + ë‹µë³€ ìƒì„±"""
    global chain
    
    if chain is None:
        raise HTTPException(status_code=500, detail="LangChainì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        logger.info(f"ğŸ“¥ ìš”ì²­ ìˆ˜ì‹ : {request.message}")

        payload = {
            "question": request.message,
            "user_name": request.userMeta.get("name", ""),
            "user_years": request.userMeta.get("years", ""),
            "user_location": request.userMeta.get("location", ""),
            "user_employees": request.userMeta.get("employees", ""),
            "user_sales": request.userMeta.get("sales", ""),
            "user_industry": request.userMeta.get("industry", ""),      # ì—…ì¢… 
            "user_types": ", ".join(request.userTypes) 
        }
        
        logger.info(f"ğŸ§ª invoke payload: {payload}")

        # Step 1: ì§ˆë¬¸ ë¶„ë¥˜
        mode = await asyncio.to_thread(classifier_chain.invoke, {"question": request.message})
        mode = mode.strip().lower()
        payload["mode"] = mode
        logger.info(f"ğŸ§  ë¶„ë¥˜ëœ ëª¨ë“œ: {mode}")
        
        # ë¹„ë™ê¸° ì‹¤í–‰
        response = await asyncio.to_thread(chain.invoke, payload)
        
        logger.info(f"ğŸ” response type: {type(response)}", extra={"flush": True})
        logger.info(f"ğŸ” response raw: {response}", extra={"flush": True})
        
        # ë°˜í™˜ í˜•íƒœê°€ ë³µì¡í•œ ì²´ì¸ì¼ìˆ˜ë¡ dictë¡œ ì‘ë‹µ
        # [ì¤„ë°”ê¿ˆ ê°€ê³µ]
        if isinstance(response, str):
            formatted_response = response.replace("\n\n", "<br><br>").replace("\n", "<br>")
        elif isinstance(response, dict):
            value = response.get("result") or response.get("output") or response.get("text")
            formatted_response = value.replace("\n\n", "<br><br>").replace("\n", "<br>") if isinstance(value, str) else str(response)
        else:
            formatted_response = str(response)
            
        return {
            "response": formatted_response,
            "croomIdx": request.croomIdx,
            "chatter": request.chatter,
            "ratings": request.ratings,
            "createdAt": request.createdAt
        }

    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def create_chain():
    """Create and return the LangChain chain with RAG, including document title."""
    try:
        # Initialize embeddings model
        embeddings_model = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-nli',
            model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
            encode_kwargs={'normalize_embeddings': True},
        )
        logger.info("âœ… Embeddings model initialized.")

        # Initialize ChromaDB
        logger.info("ğŸ” Loading ChromaDB...")
        vectorstore = Chroma(
            persist_directory="./chroma_db",  # ë””ìŠ¤í¬ ê¸°ë°˜ ì €ì¥ì†Œ ì‚¬ìš©
            embedding_function=embeddings_model,
            collection_metadata={
            "hnsw:space": "cosine",             # ìì—°ì–´ ìœ ì‚¬ë„ ì¸¡ì •ì— ê°€ì¥ ì í•©
            "hnsw:construction_ef": 400,        # ì¸ë±ìŠ¤ í’ˆì§ˆ â†‘ (ê¸°ë³¸ 200 â†’ 400)
            "hnsw:M": 32,                       # ì—°ê²° ìˆ˜ ì¦ê°€ â†’ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ (ê¸°ë³¸ 16)
            "hnsw:search_ef": 128,              # ê²€ìƒ‰ ì‹œ ì •í™•ë„ ì¦ê°€ (ê¸°ë³¸ 10~100)
            "hnsw:num_threads": 4,              # ë©€í‹°ìŠ¤ë ˆë“œ ì¸ë±ì‹± (CPU ì„±ëŠ¥ì— ë§ê²Œ ì¡°ì ˆ)
            "hnsw:batch_size": 64               # ë²¡í„° ì‚½ì… ì†ë„ í–¥ìƒ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ)
            }
        )
        
        logger.info("âœ… ChromaDB loaded successfully.")

        # MMR Retriever ì ìš©
        retriever = vectorstore.as_retriever(
            search_type="mmr",              # MMR ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
            search_kwargs={
                "k": 8,                    # ì „ì²´ í›„ë³´ ë¬¸ì„œ ìˆ˜
                "fetch_k": 20,              # ë” ë§ì€ í›„ë³´ ì¤‘ ë‹¤ì–‘ì„± ê³ ë ¤í•´ kê°œ ì„ íƒ
                "lambda_mult": 0.3          # ìœ ì‚¬ë„ 70%, ë‹¤ì–‘ì„± 30% ë°˜ì˜ (1.0: ìœ ì‚¬ë„ë§Œ, 0.0: ë‹¤ì–‘ì„±ë§Œ)
            }
        )

        # Define prompt
        prompt = PromptTemplate.from_template(
            """
            You are an AI assistant that supports government program analysis for companies.
            The user's intent has been classified as: {mode}
            
            If the mode is `recommend`, follow these instructions:

            - Recommend the most relevant government support programs for the company using the retrieved documents and the company's profile.
            - Use the following metadata fields (if present) to construct complete Korean sentences:
            - Program Name (pblancNm)
            - Application Period (reqstBeginEndDe)
            - Business Overview Contents (bsnsSumryCn)
            - Target Audience (trgetNm)
            - Program URL (pblancUrl)
            - Include key details from `page_content` such as eligibility, scope, funding, or specific support terms.
            - Do not skip any available metadata fields. Write them out in full sentences.
            - If no relevant programs are found, say so clearly. Otherwise, do not mention this.
            - At the end of each program, show the full URL prefixed with ğŸ‘‰. If `pblancUrl` is a relative path (e.g., starts with `/web/...`), prepend `https://www.bizinfo.go.kr`.

            If the mode is `summarize`, follow these instructions:

            - Provide a summary of the support programs found in the documents, regardless of company information.
            - Focus on condensing the key content from `page_content`, including funding, purpose, and eligibility.
            - Do not include user metadata or match recommendations.
            - You must include all available metadata fields (pblancNm, reqstBeginEndDe, bsnsSumryCn, trgetNm, pblancUrl) in full Korean sentences.
            - End each program's summary with the full link prefixed by ğŸ‘‰, converting relative URLs as noted.

            ---

            [Company Information]
            - Company Name: {user_name}
            - Business Years: {user_years}
            - Location: {user_location}
            - Number of Employees: {user_employees}
            - Annual Sales Range: {user_sales}
            - Business Industry: {user_industry}
            - Company Types: {user_types}    
            
            [User Question]
            {question}

            [Retrieved Support Program Documents]
            {context}

            Respond in Korean.
            """
        )
       
        llm = ChatOpenAI(
            openai_api_key=api_key,
            temperature=0.1,
            model_name="gpt-3.5-turbo",
            request_timeout=20,
        )

        logger.info("âœ… LangChain components initialized.")

        # Runnable ì²´ì¸ì˜ ì¡°í•©, | íŒŒì´í”„ ì—°ì‚°ìë¥¼ ì¨ì„œ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ ->  RunnableSequence
        # 1. Inputìœ¼ë¡œëŠ” ì „ì²´ dictì´ ë“¤ì–´ì™€, question ê°’ë§Œ êº¼ëƒ„
        # 2. retriever: query ë¬¸ìì—´ì„ ë°›ì•„ì„œ chroma ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰ -> ê²°ê³¼ëŠ” List[Document] ê°ì²´
        # 3. RunnableLambda(lambda docs: "\n\n".join([doc.page_content for doc in docs]))
        #    retrieverì˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¦¬ìŠ¤íŠ¸ docê°€ í•˜ë‚˜ë¡œ í•©ì³ì§
        
        # ìµœì¢… ì „ì²´ dict ì…ë ¥ -> "question"ë§Œ ì¶”ì¶œ (query string) -> Chromaì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        # -> ë¬¸ì„œ ë‚´ìš©ë§Œ êº¼ë‚´ì„œ í•˜ë‚˜ë¡œ í•©ì¹¨ -> PromptTemplateì— ì „ë‹¬ë  contextê°€ ë¨
        chain = (
            {
                "context": RunnableLambda(lambda x: x["question"]) 
                | retriever 
                | RunnableLambda(lambda docs: "\n\n".join([
                    f"[ğŸ“„ Metadata]\n{json.dumps({k: v for k, v in doc.metadata.items() if k!='file_hash'}, ensure_ascii=False, indent=2)}\nğŸ“‘ Content:\n{doc.page_content}"
                    for doc in docs
                ])),
                "question": RunnablePassthrough(),
                "user_name": RunnablePassthrough(),
                "user_years": RunnablePassthrough(),
                "user_location": RunnablePassthrough(),
                "user_employees": RunnablePassthrough(),
                "user_sales": RunnablePassthrough(),
                "user_industry": RunnablePassthrough(),   
                "user_types": RunnablePassthrough(),
                "mode": RunnablePassthrough(),
            }
            | prompt
            | llm
            | StrOutputParser()
        )

        # RunnableLambdaëŠ” **MMR retrieverê°€ ë°˜í™˜í•œ ë¬¸ì„œë“¤(List[Document])**
        # json.dumps(..., ensure_ascii=False, indent=2) : metaë¥¼ json ë¬¸ìì—´ë¡œ ë°˜í™˜
        # indent=2ëŠ” ë³´ê¸° ì¢‹ê²Œ ë“¤ì—¬ì“°ê¸°

        logger.info("ğŸš€ LangChain RAG chain created successfully.")

        return chain

    except Exception as e:
        logger.error(f"âŒ Error creating LangChain chain with RAG: {e}")
        raise RuntimeError("Failed to create LangChain chain with RAG")


## ë¶„ë¥˜ ì²´ì¸
def classifier_chain() :
    
    classification_prompt = PromptTemplate.from_template(
    """
    ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì½ê³  ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ ì •í™•íˆ ì„ íƒí•˜ì„¸ìš”:

    - recommend: ìš°ë¦¬ íšŒì‚¬ì— ì í•©í•œ ì •ë¶€ ì§€ì›ì‚¬ì—…ì„ ì¶”ì²œí•´ë‹¬ë¼ëŠ” ì§ˆë¬¸ì¸ ê²½ìš°
    - summarize: íŠ¹ì • ì§€ì›ì‚¬ì—…ì˜ ë‚´ìš©ì„ ìš”ì•½í•´ë‹¬ë¼ëŠ” ì§ˆë¬¸ì¸ ê²½ìš°

    ë°˜ë“œì‹œ ìœ„ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ê·¸ ì™¸ ë¬¸ì¥ì´ë‚˜ ì„¤ëª… ì—†ì´ `recommend` ë˜ëŠ” `summarize` ì¤‘ í•˜ë‚˜ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.

    [ì§ˆë¬¸]
    {question}
    """)
            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    llm = ChatOpenAI(
        openai_api_key=api_key,
        temperature=0.0,  # ë¶„ë¥˜ìš©ì€ ë³€ë™ì„± ì œê±°!
        model_name="gpt-3.5-turbo"
    )

    # ë¶„ë¥˜ ì²´ì¸
    classifier_chain = classification_prompt | llm | StrOutputParser()
    
    return classifier_chain


if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8002, reload=True)