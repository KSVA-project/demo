# MMR Retriever ì ìš©
# ì„ë² ë”© ëª¨ë¸ : HuggingFaceEmbeddings

# âœ… FastAPI ê´€ë ¨
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from contextlib import asynccontextmanager
import uvicorn

# âœ… LangChain - í•µì‹¬ ì²´ì¸ êµ¬ì„± ìš”ì†Œ
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# âœ… LangChain - ëª¨ë¸ ë° ë²¡í„°ìŠ¤í† ì–´
from langchain_community.chat_models import ChatOpenAI
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# âœ… Python ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime
from typing import Dict, Optional, List
import asyncio
import logging
import os
import json

# âœ… APScheduler ì¶”ê°€
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

# âœ… í¬ë¡¤ë§ ë° ë²¡í„° DB ì €ì¥ - ì‘ì—… ìŠ¤ì¼€ì¤„ë§
from crawling import run_crawling
from service import run_service

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv

# device í™•ì¸
import torch
def get_device():
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  ì ì ˆí•œ device ë°˜í™˜"""
    if torch.cuda.is_available():
        device = 'cuda'
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name} ({gpu_count}ê°œ)")
    else:
        device = 'cpu'
        print("âš ï¸ GPU ë¶ˆê°€ëŠ¥, CPU ì‚¬ìš©")
    return device

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LangChain ì²´ì¸ (ì „ì—­ ë³€ìˆ˜)
chain = None
classifier_chain_instance = None 

# ìš”ì²­ ë°ì´í„° ëª¨ë¸
class ChatRequest(BaseModel):
    message: str
    croomIdx: int
    chatter: str
    ratings: str = Field(default="5")
    createdAt: datetime = Field(default_factory=datetime.now)
    userMeta: Dict[str, Optional[str]] # ì‚¬ìš©ì ë©”íƒ€ë°ì´í„° í•„ë“œ
    userTypes: List[str] = Field(default=[]) # ì¶”ê°€: ê¸°ì—… ìœ í˜• ë¦¬ìŠ¤íŠ¸

def scheduled_job():
    logger.info("ğŸ•’ APScheduler: run_crawling + run_service ì‹¤í–‰ ì‹œì‘")
    try:
        run_crawling()
        run_service()
        logger.info("âœ… APScheduler: ì‘ì—… ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ APScheduler: ì‘ì—… ì‹¤íŒ¨ - {e}")

# ğŸŒ± lifespanìœ¼ë¡œ ì´ˆê¸°í™” ë¡œì§ êµ¬í˜„
@asynccontextmanager
async def lifespan(app: FastAPI):
    global chain
    global classifier_chain_instance
    logger.info("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ - LangChain ì´ˆê¸°í™” ì¤‘...")
    
    try:
        chain = create_chain()
        logger.info("âœ… LangChain ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"âœ… í˜„ì¬ ì²´ì¸ íƒ€ì…: {type(chain)}")
        
        classifier_chain_instance = create_classifier_chain()  # âŒ ìˆ˜ì •: í•¨ìˆ˜ëª… ë³€ê²½
        logger.info("âœ… classifier_chain ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"âœ… í˜„ì¬ ë¶„ë¥˜ ì²´ì¸ íƒ€ì…: {type(classifier_chain_instance)}")
        
    except Exception as e:
        logger.error(f"âŒ LangChain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        chain = None
        classifier_chain_instance = None
    
    # âœ… APScheduler ì‘ì—… ì‹œì‘
    scheduler = AsyncIOScheduler()
    scheduler.add_job(
        scheduled_job, 
        CronTrigger(hour=20, minute=20, timezone="Asia/Seoul")
    )
    scheduler.start()
    logger.info("ğŸ”„ APScheduler ì‹œì‘ - ë§¤ì¼ 20:20 ì‹¤í–‰")
    
    yield
    logger.info("ğŸ›‘ FastAPI ì„œë²„ ì¢…ë£Œ...")

# ğŸ FastAPI ì¸ìŠ¤í„´ìŠ¤
app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (í•„ìš” ì‹œ íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì²´ì¸ì—ì„œ ì‚¬ìš©ì ì •ë³´ ê²€ì¦ í•¨ìˆ˜
def validate_user_info(user_meta):
    """ì‚¬ìš©ì ì •ë³´ê°€ ì¶©ë¶„í•œì§€ ê²€ì¦"""
    required_fields = ["name", "industry", "employees", "location"]
    missing_fields = [field for field in required_fields if not user_meta.get(field)]
    
    if missing_fields:
        logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì‚¬ìš©ì ì •ë³´: {missing_fields}")
        return False, missing_fields
    return True, []

@app.post("/chat")
async def chat(request: ChatRequest):
    """LangChainì„ ì‚¬ìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ + ë‹µë³€ ìƒì„±"""
    global chain, classifier_chain_instance
    
    if chain is None or classifier_chain_instance is None:
        raise HTTPException(status_code=500, detail="LangChainì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        logger.info(f"ğŸ“¥ ìš”ì²­ ìˆ˜ì‹ : {request.message}")
        logger.info(f"ğŸ¢ íšŒì‚¬ ì •ë³´: {request.userMeta}")
        logger.info(f"ğŸ¯ ê¸°ì—… ìœ í˜•: {request.userTypes}")

        # âœ… ì¶”ê°€: ì‚¬ìš©ì ì •ë³´ ê²€ì¦
        is_valid, missing_fields = validate_user_info(request.userMeta)
        if not is_valid:
            return {
                "response": f"ë” ì •í™•í•œ ì¶”ì²œì„ ìœ„í•´ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ê°€ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”: {', '.join(missing_fields)}",
                "missing_fields": missing_fields,
                "croomIdx": request.croomIdx,
                "chatter": request.chatter,
                "ratings": request.ratings,
                "createdAt": request.createdAt
            }

        # Step 1: ì§ˆë¬¸ ë¶„ë¥˜
        mode = await asyncio.to_thread(classifier_chain_instance.invoke, {"question": request.message})
        mode = mode.strip().lower()
        logger.info(f"ğŸ§  ë¶„ë¥˜ëœ ëª¨ë“œ: {mode}")

        payload = {
            "question": request.message,
            "user_name": request.userMeta.get("name", ""),
            "user_years": request.userMeta.get("years", ""),
            "user_location": request.userMeta.get("location", ""),
            "user_employees": request.userMeta.get("employees", ""),
            "user_sales": request.userMeta.get("sales", ""),
            "user_industry": request.userMeta.get("industry", ""),
            "user_types": ", ".join(request.userTypes),
            "mode": mode
        }
        
        logger.info(f"ğŸ§ª invoke payload: {payload}")
        
        # ë¹„ë™ê¸° ì‹¤í–‰
        response = await asyncio.to_thread(chain.invoke, payload)
        
        logger.info(f"ğŸ” response type: {type(response)}")
        logger.info(f"ğŸ” response raw: {response}")
        
        # ë°˜í™˜ í˜•íƒœê°€ ë³µì¡í•œ ì²´ì¸ì¼ìˆ˜ë¡ dictë¡œ ì‘ë‹µ
        # [ì¤„ë°”ê¿ˆ ê°€ê³µ]
        if isinstance(response, str):
            formatted_response = response.replace("\n\n", "<br><br>").replace("\n", "<br>")
        elif isinstance(response, dict):
            value = response.get("result") or response.get("output") or response.get("text")
            formatted_response = value.replace("\n\n", "<br><br>").replace("\n", "<br>") if isinstance(value, str) else str(response)
        else:
            formatted_response = str(response)
            
        return {
            "response": formatted_response,
            "croomIdx": request.croomIdx,
            "chatter": request.chatter,
            "ratings": request.ratings,
            "createdAt": request.createdAt
        }

    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def create_chain():
    """Create and return the LangChain chain with RAG, including document title."""
    try:
        device = get_device()
        
        # Initialize embeddings model
        embeddings_model = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-nli',
            model_kwargs={'device': device},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
            encode_kwargs={'normalize_embeddings': True},
        )
        logger.info("âœ… Embeddings model initialized.")

        # Initialize ChromaDB
        logger.info("ğŸ” Loading ChromaDB...")
        vectorstore = Chroma(
            persist_directory="./chroma_db",  # ë””ìŠ¤í¬ ê¸°ë°˜ ì €ì¥ì†Œ ì‚¬ìš©
            embedding_function=embeddings_model,
            collection_metadata={
            "hnsw:space": "cosine",             # ìì—°ì–´ ìœ ì‚¬ë„ ì¸¡ì •ì— ê°€ì¥ ì í•©
            "hnsw:construction_ef": 400,        # ì¸ë±ìŠ¤ í’ˆì§ˆ â†‘ (ê¸°ë³¸ 200 â†’ 400)
            "hnsw:M": 32,                       # ì—°ê²° ìˆ˜ ì¦ê°€ â†’ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ (ê¸°ë³¸ 16)
            "hnsw:search_ef": 128,              # ê²€ìƒ‰ ì‹œ ì •í™•ë„ ì¦ê°€ (ê¸°ë³¸ 10~100)
            "hnsw:num_threads": 4,              # ë©€í‹°ìŠ¤ë ˆë“œ ì¸ë±ì‹± (CPU ì„±ëŠ¥ì— ë§ê²Œ ì¡°ì ˆ)
            "hnsw:batch_size": 64               # ë²¡í„° ì‚½ì… ì†ë„ í–¥ìƒ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ)
            }
        )
        
        logger.info("âœ… ChromaDB loaded successfully.")

        # MMR Retriever ì ìš©
        retriever = vectorstore.as_retriever(
            search_type="mmr",              # MMR ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
            search_kwargs={
                "k": 8,                    # ì „ì²´ í›„ë³´ ë¬¸ì„œ ìˆ˜
                "fetch_k": 20,              # ë” ë§ì€ í›„ë³´ ì¤‘ ë‹¤ì–‘ì„± ê³ ë ¤í•´ kê°œ ì„ íƒ
                "lambda_mult": 0.3          # ìœ ì‚¬ë„ 70%, ë‹¤ì–‘ì„± 30% ë°˜ì˜ (1.0: ìœ ì‚¬ë„ë§Œ, 0.0: ë‹¤ì–‘ì„±ë§Œ)
            }
        )

        # ëª¨ë“œë³„ ì„¸ë¶€ ì§€ì¹¨ì„ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ - ìì„¸í•œ ì •ë³´ ì œê³µ ë²„ì „
        def get_mode_instructions(mode_value):
            if mode_value == "recommend":
                return """
                ğŸ¯ ì¶”ì²œ ëª¨ë“œ ìˆ˜í–‰ì‚¬í•­:
                
                1. íšŒì‚¬ í”„ë¡œí•„ ë¶„ì„ë¶€í„° ì‹œì‘:
                   - í•´ë‹¹ ê¸°ì—…ì˜ íŠ¹ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ê³  ë¶„ì„
                   - í•´ë‹¹ ê¸°ì—…ì˜ ì—…ì¢…ê³¼ ê·œëª¨ì— íŠ¹í™”ëœ ì§€ì›ì‚¬ì—… í•„ìš”ì„± ë¶„ì„
                   - ê¸°ì—…ì˜ ì„±ì¥ ë‹¨ê³„ì™€ í˜„ì¬ ìƒí™©ì— ë§ëŠ” ì§€ì› í•„ìš”ì„± ì„¤ëª…
                
                2. ë§ì¶¤ ì¶”ì²œ ì œê³µ:
                   - ê²€ìƒ‰ëœ ê° ì§€ì›ì‚¬ì—…ì´ ì´ íšŒì‚¬ì— ì™œ ì í•©í•œì§€ êµ¬ì²´ì  ë§¤ì¹­ ê·¼ê±° ì œì‹œ
                   - ê¸°ì—… ê·œëª¨, ì—°ë§¤ì¶œ, ì—…ì¢…ê³¼ì˜ ì—°ê´€ì„± ìƒì„¸ ì„¤ëª…
                   - ì§€ì—­ ê¸°ë°˜ ì§€ì›ì‚¬ì—…ì´ ìˆë‹¤ë©´ ìš°ì„  ì¶”ì²œí•˜ê³  ì§€ì—­ì  ì¥ì  ì„¤ëª…
                   - ì‹ ì²­ ê°€ëŠ¥ì„±ê³¼ ê²½ìŸë¥ , ì„ ì • ê°€ëŠ¥ì„±ê¹Œì§€ ê³ ë ¤í•œ ì¶”ì²œ
                
                3. ê° í”„ë¡œê·¸ë¨ë³„ ìƒì„¸ ì •ë³´ (ë°˜ë“œì‹œ ëª¨ë“  í•­ëª©ì„ ìì„¸íˆ ì„¤ëª…):
                   - ì‚¬ì—…ëª… (pblancNm): "[ì‚¬ì—…ëª…]" - ì‚¬ì—…ì˜ ëª©ì ê³¼ ë°°ê²½ê¹Œì§€ ì„¤ëª…
                   - ğŸ“… ì‹ ì²­ê¸°ê°„ (reqstBeginEndDe): "[ê¸°ê°„]" - ì‹ ì²­ ë§ˆê°ì¼ê¹Œì§€ ë‚¨ì€ ì‹œê°„ê³¼ ì¤€ë¹„ ê¸°ê°„ ì•ˆë‚´
                   - ğŸ“‹ ì‚¬ì—…ê°œìš” (bsnsSumryCn): 
                     * ì‚¬ì—…ì˜ ì „ì²´ì ì¸ ëª©í‘œì™€ ë°©í–¥ì„±
                     * ì§€ì› ë‚´ìš©ì˜ êµ¬ì²´ì ì¸ ë²”ìœ„ì™€ í•œê³„
                     * ì˜ˆìƒë˜ëŠ” ì„±ê³¼ì™€ ê¸°ëŒ€ íš¨ê³¼
                     * ì‚¬ì—… ì°¸ì—¬ ì‹œ ì–»ì„ ìˆ˜ ìˆëŠ” í˜œíƒë“¤ì„ êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì—´
                   - ğŸ¯ ëŒ€ìƒ (trgetNm): "[ëŒ€ìƒ]" - ìš°ë¦¬ íšŒì‚¬ê°€ í•´ë‹¹ ëŒ€ìƒì— í¬í•¨ë˜ëŠ” ì´ìœ ì™€ ìê²© ìš”ê±´ ë¶„ì„
                   -  ì§€ì› ê·œëª¨: ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì§€ì›ê¸ˆì•¡, ì§€ì› ë¹„ìœ¨, ì§€ì› ë°©ì‹ ë“± ìƒì„¸ ì •ë³´
                   -  ì‹ ì²­ ì ˆì°¨: í•„ìš”í•œ ì„œë¥˜, ì‹¬ì‚¬ ê³¼ì •, ì„ ì • ê¸°ì¤€ ë“± ì‹¤ë¬´ì  ì •ë³´
                   -  ì£¼ì˜ì‚¬í•­: ì‹ ì²­ ì‹œ ìœ ì˜í•  ì , ì œì™¸ ëŒ€ìƒ, ì˜ë¬´ì‚¬í•­ ë“±
                   -  ë§í¬* [ì „ì²´URL]
                
                4. ìš°ì„ ìˆœìœ„ ì •ë ¬ ë° ìƒì„¸ ë¶„ì„:
                   - ê°€ì¥ ì í•©í•œ í”„ë¡œê·¸ë¨ë¶€í„° ìˆœì„œëŒ€ë¡œ ì œì‹œ
                   - ê°ê°ì— ëŒ€í•´ ì´ íšŒì‚¬ì— ì í•©í•œ ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
                   - ì‹ ì²­ ë‚œì´ë„ì™€ ì„±ê³µ ê°€ëŠ¥ì„± í‰ê°€
                   - ë™ì‹œ ì‹ ì²­ ê°€ëŠ¥í•œ í”„ë¡œê·¸ë¨ë“¤ì˜ ì¡°í•© ì œì•ˆ
                """
            else:  # summarize
                return """
                ğŸ“Š ìš”ì•½ ëª¨ë“œ ìˆ˜í–‰ì‚¬í•­:
                
                1. íšŒì‚¬ ë§¥ë½ì—ì„œì˜ ìƒì„¸ ìš”ì•½:
                   - ì¼ë°˜ì  ìš”ì•½ì´ ì•„ë‹Œ, í•´ë‹¹ ì—…ì¢… ê¸°ì—… ê´€ì ì—ì„œ ìƒì„¸ ìš”ì•½
                   - í•´ë‹¹ ê·œëª¨ ê¸°ì—…ì´ í™œìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ê´€ì ê³¼ ê°€ëŠ¥ì„± ë¶„ì„
                   - ë¹„ìŠ·í•œ ê·œëª¨/ì—…ì¢… ê¸°ì—…ì˜ í™œìš© ì‚¬ë¡€ë‚˜ ì„±ê³µ íŒ¨í„´ ì–¸ê¸‰ (ë¬¸ì„œì— ìˆë‹¤ë©´)
                
                2. í•µì‹¬ ì •ë³´ ìƒì„¸ êµ¬ì¡°í™”:
                   -  **ì‚¬ì—…ëª… (pblancNm)**: "[ì‚¬ì—…ëª…]" - ì‚¬ì—…ëª…ì˜ ì˜ë¯¸ì™€ ë°°ê²½ ì„¤ëª…
                   -  **ì‹ ì²­ê¸°ê°„ (reqstBeginEndDe)**: "[ê¸°ê°„]" - ì‹ ì²­ ì¼ì •ê³¼ ê´€ë ¨ ì¤‘ìš” ë‚ ì§œë“¤
                   -  **ì‚¬ì—…ê°œìš” (bsnsSumryCn)**: 
                     * ì‚¬ì—…ì˜ ì „ì²´ ëª©í‘œì™€ ì¶”ì§„ ë°°ê²½
                     * ì§€ì› ë‚´ìš©ì˜ ìƒì„¸ ë²”ìœ„ (ìê¸ˆì§€ì›, ë©˜í† ë§, êµìœ¡, ë„¤íŠ¸ì›Œí‚¹ ë“±)
                     * ì‚¬ì—… ê¸°ê°„ê³¼ ë‹¨ê³„ë³„ ì§„í–‰ ê³¼ì •
                     * ì°¸ì—¬ ê¸°ì—…ì´ ì–»ê²Œ ë˜ëŠ” êµ¬ì²´ì  í˜œíƒë“¤
                     * ì˜ë¬´ì‚¬í•­ê³¼ ì¡°ê±´ë“¤
                   -  ëŒ€ìƒ (trgetNm): "[ëŒ€ìƒ]" - ëŒ€ìƒ ì¡°ê±´ì˜ ìƒì„¸ ë¶„ì„ê³¼ ìš°ë¦¬ íšŒì‚¬ ì í•©ì„±
                   -  ì§€ì› ì¡°ê±´: ì§€ì›ê¸ˆì•¡, ì§€ì› ë¹„ìœ¨, ë§¤ì¹­í€ë“œ ì—¬ë¶€ ë“± ì¬ì •ì  ì¡°ê±´
                   -  ì‹ ì²­ ìš”ê±´: í•„ìš” ì„œë¥˜, ìê²© ì¡°ê±´, ì œì¶œ ë°©ë²• ë“±
                   -  ì‹¬ì‚¬ ê¸°ì¤€: í‰ê°€ í•­ëª©, ê°€ì  ìš”ì†Œ, ìš°ëŒ€ ì¡°ê±´ ë“±
                   -  ë§í¬: [ì „ì²´URL]
                
                3. ì‹¤ìš©ì  ìƒì„¸ ë¶„ì„:
                   - ì§€ì› ê·œëª¨, ì¡°ê±´, ì ˆì°¨ ë“± ëª¨ë“  í•µì‹¬ ì‹¤ë¬´ ì •ë³´
                   - ì´ íšŒì‚¬ê°€ ì§€ì› ê°€ëŠ¥í•œì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì •ë³´ ì œê³µ
                   - ì‹ ì²­ ì „ ì¤€ë¹„í•´ì•¼ í•  ì‚¬í•­ë“¤ê³¼ ì†Œìš” ì‹œê°„ ì˜ˆìƒ
                   - ì„ ì • í›„ ì§„í–‰ ê³¼ì •ê³¼ ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë¬¼
                   - ë‹¤ë¥¸ ì§€ì›ì‚¬ì—…ê³¼ì˜ ì¤‘ë³µ ì§€ì› ê°€ëŠ¥ ì—¬ë¶€
                """

        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ - ì‚¬ìš©ì ì •ë³´ í™œìš© ê°•í™”
         # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ - ë”ìš± ìì„¸í•œ ì •ë³´ ì œê³µ
        prompt = PromptTemplate.from_template(
            """
            ë‹¹ì‹ ì€ ì •ë¶€ì§€ì›ì‚¬ì—… ì „ë¬¸ AI ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. 
            ì‚¬ìš©ìì˜ ì§ˆë¬¸ ìœ í˜•: {mode}
            
            ë°˜ë“œì‹œ ì•„ë˜ íšŒì‚¬ ì •ë³´ë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:
            
            ğŸ¢ ë¶„ì„í•  íšŒì‚¬ ì •ë³´
            - íšŒì‚¬ëª…: {user_name}
            - ì‚¬ì—… ì—°ìˆ˜: {user_years}ë…„ â† ì´ ì—…ì¢…ì„ ë°˜ë“œì‹œ ê³ ë ¤í•˜ì„¸ìš”!
            - ì†Œì¬ì§€: {user_location}   â† ì´ ì†Œì¬ì§€ì„ ë°˜ë“œì‹œ ê³ ë ¤í•˜ì„¸ìš”!
            - ì§ì› ìˆ˜: {user_employees}ëª…
            - ì—°ë§¤ì¶œ ê·œëª¨: {user_sales}
            - ì—…ì¢…: {user_industry}
            - ê¸°ì—… ìœ í˜•: {user_types}
            
            ğŸ“‹ ì‚¬ìš©ì ì§ˆë¬¸
            {question}

            ---
            
            '{mode}' ëª¨ë“œ ìˆ˜í–‰ ì§€ì¹¨:
            
            {mode_instructions}
            
            ğŸ“‘ ê²€ìƒ‰ëœ ì •ë¶€ì§€ì›ì‚¬ì—… ë¬¸ì„œ:
            {context}
            
            âš ï¸ ì¤‘ìš” ì§€ì¹¨ (ìì„¸í•œ ì •ë³´ ì œê³µ):
            1. ë°˜ë“œì‹œ ìœ„ íšŒì‚¬ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ë¶„ì„ì„ ì œê³µí•˜ë˜, ëª¨ë“  ì •ë³´ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”
            2. íšŒì‚¬ì˜ ì—…ì¢…, ê·œëª¨, ì§€ì—­ì„ ê³ ë ¤í•œ êµ¬ì²´ì  ë§¤ì¹­ ì´ìœ ë¥¼ ìì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”
            3. ì¼ë°˜ì ì¸ ë‹µë³€ì´ ì•„ë‹Œ, í•´ë‹¹ ê¸°ì—…ë§Œì„ ìœ„í•œ íŠ¹í™”ëœ ìƒì„¸ ë‹µë³€ì„ í•˜ì„¸ìš”
            4. ë©”íƒ€ë°ì´í„°ì˜ ëª¨ë“  í•„ë“œë¥¼ í•œêµ­ì–´ë¡œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ë‹¨ìˆœ ë‚˜ì—´ì´ ì•„ë‹Œ ì„¤ëª…í˜•ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
            5. ì‚¬ì—…ê°œìš”(bsnsSumryCn)ëŠ” íŠ¹íˆ ìì„¸íˆ í’€ì–´ì„œ ì„¤ëª…í•˜ê³ , ì‹¤ë¬´ì§„ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
            6. ì§€ì› ì¡°ê±´, ì‹ ì²­ ë°©ë²•, ì‹¬ì‚¬ ê¸°ì¤€ ë“± ì‹¤ë¬´ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
            7. URLì€ ğŸ‘‰ ë§ˆí¬ì™€ í•¨ê»˜ ì œê³µí•˜ê³ , ìƒëŒ€ê²½ë¡œëŠ” https://www.bizinfo.go.krë¥¼ ì•ì— ë¶™ì´ì„¸ìš”
            8. ê° ì§€ì›ì‚¬ì—…ì— ëŒ€í•´ "ì™œ ì´ íšŒì‚¬ì— ë„ì›€ì´ ë˜ëŠ”ì§€"ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
            9. ì‹ ì²­ ì‹œ ì˜ˆìƒë˜ëŠ” ê²½ìŸë¥ ì´ë‚˜ ì„ ì • ê°€ëŠ¥ì„±ë„ ì–¸ê¸‰í•˜ì„¸ìš” (ë¬¸ì„œì— ì •ë³´ê°€ ìˆë‹¤ë©´)
            10. ì ˆëŒ€ë¡œ ì •ë³´ë¥¼ ì¶•ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ê³ , ë¬¸ì„œì— ìˆëŠ” ëª¨ë“  ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
            
            í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
            """
        )

        llm = ChatOpenAI(
            openai_api_key=api_key,
            temperature=0.0,
            model_name="gpt-3.5-turbo",
            request_timeout=30,
            max_tokens=2000,
        )

        logger.info("âœ… LangChain components initialized.")

        # ì²´ì¸ êµ¬ì„± ì‹œ ëª¨ë“œë³„ ì§€ì¹¨ì„ ë™ì ìœ¼ë¡œ ì‚½ì…
        chain = (
            {
                "context": RunnableLambda(lambda x: x["question"]) 
                | retriever 
                | RunnableLambda(lambda docs: "\n\n".join([
                    f"ğŸ“„ **ë¬¸ì„œ {i+1}**\n"
                    f"ğŸ“‹ **ë©”íƒ€ë°ì´í„°:**\n{json.dumps({k: v for k, v in doc.metadata.items() if k!='file_hash'}, ensure_ascii=False, indent=2)}\n\n"
                    f"ğŸ“‘ **ë‚´ìš©:**\n{doc.page_content}\n"
                    f"{'='*50}"
                    for i, doc in enumerate(docs)
                ])),
                "question": RunnablePassthrough(),
                "user_name": RunnablePassthrough(),
                "user_years": RunnablePassthrough(),
                "user_location": RunnablePassthrough(),
                "user_employees": RunnablePassthrough(),
                "user_sales": RunnablePassthrough(),
                "user_industry": RunnablePassthrough(),   
                "user_types": RunnablePassthrough(),
                "mode": RunnablePassthrough(),
                "mode_instructions": RunnableLambda(lambda x: get_mode_instructions(x.get("mode", "recommend")))
            }
            | prompt
            | llm
            | StrOutputParser()
        )

        logger.info("ğŸš€ ê°œì„ ëœ LangChain RAG chain ìƒì„± ì™„ë£Œ")
        return chain

    except Exception as e:
        logger.error(f"âŒ Error creating improved LangChain chain: {e}")
        raise RuntimeError("Failed to create improved LangChain chain")


# ê°œì„ ëœ ë¶„ë¥˜ ì²´ì¸
def create_classifier_chain():  # âŒ ìˆ˜ì •: í•¨ìˆ˜ëª… ë³€ê²½
    classification_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.
        
        **ë¶„ë¥˜ ê¸°ì¤€:**
        - `recommend`: 
          * "ì¶”ì²œí•´ì£¼ì„¸ìš”", "ì í•©í•œ", "ìš°ë¦¬ íšŒì‚¬ì—", "ë§ëŠ”", "ì°¾ì•„ì£¼ì„¸ìš”" ë“±ì˜ í‘œí˜„
          * íšŒì‚¬ ìƒí™©ì— ë§ëŠ” ì§€ì›ì‚¬ì—…ì„ ì°¾ì•„ë‹¬ë¼ëŠ” ìš”ì²­
          * ì˜ˆ: "ìš°ë¦¬ íšŒì‚¬ì— ë§ëŠ” ì •ë¶€ì§€ì›ì‚¬ì—… ì¶”ì²œí•´ì£¼ì„¸ìš”"
          
        - `summarize`:
          * "ìš”ì•½í•´ì£¼ì„¸ìš”", "ì •ë¦¬í•´ì£¼ì„¸ìš”", "ì„¤ëª…í•´ì£¼ì„¸ìš”", "ì•Œë ¤ì£¼ì„¸ìš”" ë“±ì˜ í‘œí˜„
          * íŠ¹ì • í”„ë¡œê·¸ë¨ì´ë‚˜ ì¼ë°˜ì ì¸ ì •ë³´ì˜ ìš”ì•½ì„ ì›í•˜ëŠ” ìš”ì²­
          * ì˜ˆ: "ì°½ì—…ì§€ì›ì‚¬ì—…ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"

        **ì¤‘ìš”:** ì•„ë˜ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš”.
        - recommend
        - summarize

        ì§ˆë¬¸: {question}
        
        ë‹µë³€:"""
    )
    
    llm = ChatOpenAI(
        openai_api_key=api_key,
        temperature=0.0,
        model_name="gpt-3.5-turbo"
    )

    return classification_prompt | llm | StrOutputParser()


if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8002, reload=True)