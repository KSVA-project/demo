# MMR Retriever ì ìš©
# ì„ë² ë”© ëª¨ë¸ : OpenAIEmbeddings

# âœ… FastAPI ê´€ë ¨
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from contextlib import asynccontextmanager
import uvicorn

# âœ… LangChain - í•µì‹¬ ì²´ì¸ êµ¬ì„± ìš”ì†Œ
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# âœ… LangChain - ëª¨ë¸ ë° ë²¡í„°ìŠ¤í† ì–´
from langchain_community.chat_models import ChatOpenAI
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# âœ… Python ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime
from typing import Dict, Optional
import asyncio
import logging
import os
import json

# âœ… APScheduler ì¶”ê°€
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

# âœ… í¬ë¡¤ë§ ë° ë²¡í„° DB ì €ì¥ - ì‘ì—… ìŠ¤ì¼€ì¤„ë§
from crawling import run_crawling
from service import run_service

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LangChain ì²´ì¸ (ì „ì—­ ë³€ìˆ˜)
chain = None

# ìš”ì²­ ë°ì´í„° ëª¨ë¸
class ChatRequest(BaseModel):
    message: str
    croomIdx: int
    chatter: str
    ratings: str = Field(default="5")
    createdAt: datetime = Field(default_factory=datetime.now)
    userMeta: Dict[str, Optional[str]] # ì‚¬ìš©ì ë©”íƒ€ë°ì´í„° í•„ë“œ


def scheduled_job():
    logger.info("ğŸ•’ APScheduler: run_crawling + run_service ì‹¤í–‰ ì‹œì‘")
    try:
        run_crawling()
        run_service()
        logger.info("âœ… APScheduler: ì‘ì—… ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ APScheduler: ì‘ì—… ì‹¤íŒ¨ - {e}")

# ğŸŒ± lifespanìœ¼ë¡œ ì´ˆê¸°í™” ë¡œì§ êµ¬í˜„
@asynccontextmanager
async def lifespan(app: FastAPI):
    global chain
    logger.info("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ - LangChain ì´ˆê¸°í™” ì¤‘...")
    
    try:
        chain = create_chain()
        logger.info("âœ… LangChain ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"âœ… í˜„ì¬ ì²´ì¸ íƒ€ì…: {type(chain)}")
        logger.info(f"âœ… ì²´ì¸ êµ¬ì„±: {chain}")
        
    except Exception as e:
        logger.error(f"âŒ LangChain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        chain = None
    
    # âœ… APScheduler ì‘ì—… ì‹œì‘
    scheduler = AsyncIOScheduler()
    scheduler.add_job(
        scheduled_job, 
        CronTrigger(hour=20, minute=20, timezone="Asia/Seoul")
    )
    scheduler.start()
    logger.info("ğŸ”„ APScheduler ì‹œì‘ - ë§¤ì¼ 20:20 ì‹¤í–‰")
    
    yield
    logger.info("ğŸ›‘ FastAPI ì„œë²„ ì¢…ë£Œ...")


# ğŸ FastAPI ì¸ìŠ¤í„´ìŠ¤
app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (í•„ìš” ì‹œ íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/chat")
async def chat(request: ChatRequest):
    """LangChainì„ ì‚¬ìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ + ë‹µë³€ ìƒì„±"""
    global chain
    
    if chain is None:
        raise HTTPException(status_code=500, detail="LangChainì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        logger.info(f"ğŸ“¥ ìš”ì²­ ìˆ˜ì‹ : {request.message}")

        payload = {
            "question": request.message,
            "user_name": request.userMeta.get("name", ""),
            "user_years": request.userMeta.get("years", ""),
            "user_location": request.userMeta.get("location", ""),
            "user_employees": request.userMeta.get("employees", ""),
            "user_sales": request.userMeta.get("sales", "")
        }
        
        logger.info(f"ğŸ§ª invoke payload: {payload}")


        # ë¹„ë™ê¸° ì‹¤í–‰
        response = await asyncio.to_thread(chain.invoke, payload)
        
        logger.info(f"ğŸ” response type: {type(response)}", extra={"flush": True})
        logger.info(f"ğŸ” response raw: {response}", extra={"flush": True})
        
        # ë°˜í™˜ í˜•íƒœê°€ ë³µì¡í•œ ì²´ì¸ì¼ìˆ˜ë¡ dictë¡œ ì‘ë‹µ
        # [ì¤„ë°”ê¿ˆ ê°€ê³µ]
        if isinstance(response, str):
            formatted_response = response.replace("\n\n", "<br><br>").replace("\n", "<br>")
        elif isinstance(response, dict):
            value = response.get("result") or response.get("output") or response.get("text")
            formatted_response = value.replace("\n\n", "<br><br>").replace("\n", "<br>") if isinstance(value, str) else str(response)
        else:
            formatted_response = str(response)
            
        return {
            "response": formatted_response,
            "croomIdx": request.croomIdx,
            "chatter": request.chatter,
            "ratings": request.ratings,
            "createdAt": request.createdAt
        }

    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def create_chain():
    """Create and return the LangChain chain with RAG, including document title."""
    try:
        # Initialize embeddings model
        embeddings_model = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-nli',
            model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
            encode_kwargs={'normalize_embeddings': True},
        )
        logger.info("âœ… Embeddings model initialized.")

        # Initialize ChromaDB
        logger.info("ğŸ” Loading ChromaDB...")
        vectorstore = Chroma(
            persist_directory="./chroma_db",  # ë””ìŠ¤í¬ ê¸°ë°˜ ì €ì¥ì†Œ ì‚¬ìš©
            embedding_function=embeddings_model,
            collection_metadata={
            "hnsw:space": "cosine",             # ìì—°ì–´ ìœ ì‚¬ë„ ì¸¡ì •ì— ê°€ì¥ ì í•©
            "hnsw:construction_ef": 400,        # ì¸ë±ìŠ¤ í’ˆì§ˆ â†‘ (ê¸°ë³¸ 200 â†’ 400)
            "hnsw:M": 32,                       # ì—°ê²° ìˆ˜ ì¦ê°€ â†’ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ (ê¸°ë³¸ 16)
            "hnsw:search_ef": 128,              # ê²€ìƒ‰ ì‹œ ì •í™•ë„ ì¦ê°€ (ê¸°ë³¸ 10~100)
            "hnsw:num_threads": 4,              # ë©€í‹°ìŠ¤ë ˆë“œ ì¸ë±ì‹± (CPU ì„±ëŠ¥ì— ë§ê²Œ ì¡°ì ˆ)
            "hnsw:batch_size": 64               # ë²¡í„° ì‚½ì… ì†ë„ í–¥ìƒ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ)
            }
        )
        
        logger.info("âœ… ChromaDB loaded successfully.")

        # MMR Retriever ì ìš©
        retriever = vectorstore.as_retriever(
            search_type="mmr",              # MMR ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
            search_kwargs={
                "k": 8,                    # ì „ì²´ í›„ë³´ ë¬¸ì„œ ìˆ˜
                "fetch_k": 20,              # ë” ë§ì€ í›„ë³´ ì¤‘ ë‹¤ì–‘ì„± ê³ ë ¤í•´ kê°œ ì„ íƒ
                "lambda_mult": 0.3          # ìœ ì‚¬ë„ 70%, ë‹¤ì–‘ì„± 30% ë°˜ì˜ (1.0: ìœ ì‚¬ë„ë§Œ, 0.0: ë‹¤ì–‘ì„±ë§Œ)
            }
        )

        # Define prompt
        prompt = PromptTemplate.from_template(
            """
            You are an AI assistant that recommends suitable government support programs for companies.

            You must perform two tasks:
            1. Recommend the most relevant support programs for the company based on the retrieved documents and the provided company profile.
            2. Summarize the key details of the selected support programs in a clear and concise manner.

            The metadata included in each document (such as program name, field, region, contact information, url, cost etc.) is critical.
            Please analyze and utilize it to generate more accurate recommendations and summaries.

            If none of the programs match the company's profile, say you couldn't find a suitable one.
            **However, if any relevant programs are recommended, do not include such a message.**


            [Company Information]
            - Company Name: {user_name}
            - Business Years: {user_years}
            - Location: {user_location}
            - Number of Employees: {user_employees}
            - Annual Sales Range: {user_sales}

            [User Question]
            {question}

            [Retrieved Support Program Documents]
            {context}

            Answer in Korean.
            """
        )
       
        llm = ChatOpenAI(
            openai_api_key=api_key,
            temperature=0.1,
            model_name="gpt-3.5-turbo",
            request_timeout=20,
        )

        logger.info("âœ… LangChain components initialized.")

        # Runnable ì²´ì¸ì˜ ì¡°í•©, | íŒŒì´í”„ ì—°ì‚°ìë¥¼ ì¨ì„œ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ ->  RunnableSequence
        # 1. Inputìœ¼ë¡œëŠ” ì „ì²´ dictì´ ë“¤ì–´ì™€, question ê°’ë§Œ êº¼ëƒ„
        # 2. retriever: query ë¬¸ìì—´ì„ ë°›ì•„ì„œ chroma ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰ -> ê²°ê³¼ëŠ” List[Document] ê°ì²´
        # 3. RunnableLambda(lambda docs: "\n\n".join([doc.page_content for doc in docs]))
        #    retrieverì˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¦¬ìŠ¤íŠ¸ docê°€ í•˜ë‚˜ë¡œ í•©ì³ì§
        
        # ìµœì¢… ì „ì²´ dict ì…ë ¥ -> "question"ë§Œ ì¶”ì¶œ (query string) -> Chromaì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        # -> ë¬¸ì„œ ë‚´ìš©ë§Œ êº¼ë‚´ì„œ í•˜ë‚˜ë¡œ í•©ì¹¨ -> PromptTemplateì— ì „ë‹¬ë  contextê°€ ë¨
        chain = (
            {
                "context": RunnableLambda(lambda x: x["question"]) 
                | retriever 
                | RunnableLambda(lambda docs: "\n\n".join([
                    f"[ğŸ“„ Metadata]\n{json.dumps({k: v for k, v in doc.metadata.items() if k!='file_hash'}, ensure_ascii=False, indent=2)}\nğŸ“‘ Content:\n{doc.page_content}"
                    for doc in docs
                ])),
                "question": RunnablePassthrough(),
                "user_name": RunnablePassthrough(),
                "user_years": RunnablePassthrough(),
                "user_location": RunnablePassthrough(),
                "user_employees": RunnablePassthrough(),
                "user_sales": RunnablePassthrough(),
            }
            | prompt
            | llm
            | StrOutputParser()
        )

        # RunnableLambdaëŠ” **MMR retrieverê°€ ë°˜í™˜í•œ ë¬¸ì„œë“¤(List[Document])**
        # json.dumps(..., ensure_ascii=False, indent=2) : metaë¥¼ json ë¬¸ìì—´ë¡œ ë°˜í™˜
        # indent=2ëŠ” ë³´ê¸° ì¢‹ê²Œ ë“¤ì—¬ì“°ê¸°

        logger.info("ğŸš€ LangChain RAG chain created successfully.")

        return chain

    except Exception as e:
        logger.error(f"âŒ Error creating LangChain chain with RAG: {e}")
        raise RuntimeError("Failed to create LangChain chain with RAG")


if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8002, reload=True)