{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 파이썬 가상환경 생성\n",
    "python -m venv .venv\n",
    "  \n",
    "(.venv) = 설정하고자 가상환경 이름\n",
    "\n",
    "# 주피터 커널 등록\n",
    "python -m ipykernel install --user --name=venv --display-name \"Python (.venv)\n",
    "(Python) = 설정하고자 하는 가상환경 이름\n",
    "\n",
    "\n",
    "PowerShell에서 가상환경 활성화\n",
    ".\\.venv\\Scripts\\Activate.PS1\n",
    "\n",
    "가상환경 리스트 확인 명령어\n",
    "jupyter kernelspec list\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c96f0",
   "metadata": {},
   "source": [
    "### PyMuPDF + Camelot(Lattice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c609d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadError(\"Invalid Elementary Object starting with b')' @179086: b'ttp://smart.nipa.kr))\\\\n/S /URI >>\\\\n/H /I\\\\n/F 28\\\\n>>\\\\nendobj\\\\n311 0 obj\\\\n<< /Type /Annot'\")\n",
      "PdfReadError(\"Invalid Elementary Object starting with b')' @179278: b'(http://www.nipa.kr))\\\\n/S /URI >>\\\\n/H /I\\\\n/F 28\\\\n>>\\\\nendobj\\\\n309 0 obj\\\\n[\\\\n308 0 R\\\\n310 0'\")\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_23468\\2052424735.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = table.df.applymap(lambda x: x.replace('\\n', ' ') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import fitz\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "def extract_text_with_fitz(pdf_path):\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    texts = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        texts.append(text.strip() if text else \"\")\n",
    "        \n",
    "    return texts\n",
    "\n",
    "def extract_tables_lattice_per_page(pdf_path):\n",
    "    # 페이지별 표를 저장할 딕셔너리 {페이지번호(1-based): [DataFrame, ...], ...}\n",
    "    tables_per_page = {}\n",
    "\n",
    "    # Camelot으로 모든 표를 추출 (페이지별로 추출을 못하니 각 표의 페이지 정보 이용)\n",
    "    tables = camelot.read_pdf(pdf_path, pages=\"1-end\", flavor=\"lattice\")\n",
    "    \n",
    "    for table in tables:\n",
    "        page_num = table.page  # Camelot이 알려주는 페이지 번호 (1-based)\n",
    "        df = table.df.applymap(lambda x: x.replace('\\n', ' ') if isinstance(x, str) else x)\n",
    "        if page_num not in tables_per_page:\n",
    "            tables_per_page[page_num] = []\n",
    "        tables_per_page[page_num].append(df)\n",
    "        \n",
    "\n",
    "    return tables_per_page\n",
    "\n",
    "def convert_tables_to_markdown(tables):\n",
    "    markdowns = []\n",
    "    for i, df in enumerate(tables):\n",
    "        try:\n",
    "            markdown_table = tabulate(df.values.tolist(), headers=df.iloc[0].tolist(), tablefmt=\"github\")\n",
    "            markdowns.append(f\"▼ 표 {i + 1}:\\n{markdown_table}\")\n",
    "        except Exception as e:\n",
    "            markdowns.append(f\"▼ 표 {i + 1}: 변환 실패 ({e})\")\n",
    "    return \"\\n\\n\".join(markdowns)\n",
    "            \n",
    "\n",
    "# 경로\n",
    "pdf_path = \"./data/2025년 AI반도체 조기 상용화 및 AX실증 지원 사업 통합 공고문.pdf\"\n",
    "\n",
    "# 파일명, 확장자 분리\n",
    "file_with_ext = os.path.basename(pdf_path)\n",
    "file_name, file_ext = os.path.splitext(file_with_ext)\n",
    "\n",
    "# 텍스트 추출\n",
    "texts = extract_text_with_fitz(pdf_path)\n",
    "\n",
    "# 페이지별 표 추출\n",
    "tables_per_page= extract_tables_lattice_per_page(pdf_path)\n",
    "\n",
    "\n",
    "# 페이지별 document 리스트 생성\n",
    "documents = []\n",
    "total_pages = len(texts)\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    page_num = i + 1\n",
    "    tables = tables_per_page.get(page_num, [])\n",
    "    \n",
    "    tables_Str = convert_tables_to_markdown(tables)\n",
    "    combined_text = text\n",
    "    \n",
    "    if tables_Str:\n",
    "        combined_text += \"\\n\\n---\\n\\n📊 페이지 내 표 정보:\\n\" + tables_Str\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=combined_text, \n",
    "        metadata={\"page\": page_num,\n",
    "                  \"title\": file_name\n",
    "                  }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "# 예시 출력\n",
    "with open(\"test/pdf.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(documents[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350ae6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 1, 'title': '2025년 AI반도체 조기 상용화 및 AX실증 지원 사업 통합 공고문'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  단계 2: 문서 분할\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter()\n",
    "\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "split_documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99da2f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\KSVA\\Project\\python\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-nli', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단계 3: 임베딩 생성\n",
    "\n",
    "# sentence-transformers 라이브러리를 사용하면  (위에서 정의함)\n",
    "# HuggingFace 모델에서 사용된 사전 훈련된 임베딩 모델을 다운로드 받아서 적용\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='jhgan/ko-sroberta-nli', # 한국어 자연어 추론 최족화된  ko-sroberta 모델\n",
    "    model_kwargs={'device':'cpu'}, # CPU에서 실행되도록 설정\n",
    "    encode_kwargs={'normalize_embeddings':True}, # 임베딩을 정규화, 벡터가 같은 범위의 값을 갖도록 함. (유사도 계산시 일관성 높임)\n",
    ")\n",
    "\n",
    "embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f50b407",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Chroma 벡터 스토어에 문서와 메타데이터 저장\u001b[39;00m\n\u001b[32m      5\u001b[39m vectorstore = Chroma.from_documents(\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     documents=\u001b[43msplit_documents\u001b[49m,\n\u001b[32m      7\u001b[39m     embedding=embeddings_model,\n\u001b[32m      8\u001b[39m     persist_directory=\u001b[33m\"\u001b[39m\u001b[33m./chroma_db\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# 벡터 스토어를 디스크에 저장\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m vectorstore\n",
      "\u001b[31mNameError\u001b[39m: name 'split_documents' is not defined"
     ]
    }
   ],
   "source": [
    "# 단계 4 : DB 생성 ChromaDB 및 저장\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Chroma 벡터 스토어에 문서와 메타데이터 저장\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\"  # 벡터 스토어를 디스크에 저장\n",
    ")\n",
    "\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0310e311",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 단계 5: 검색기(Retriver) 생성\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m retriever = \u001b[43mvectorstore\u001b[49m.as_retriever()\n\u001b[32m      3\u001b[39m retriever\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "# 단계 5: 검색기(Retriver) 생성\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9700ae6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# MMR Retriever 적용\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m retriever = \u001b[43mvectorstore\u001b[49m.as_retriever(\n\u001b[32m      3\u001b[39m     search_type=\u001b[33m\"\u001b[39m\u001b[33mmmr\u001b[39m\u001b[33m\"\u001b[39m,              \u001b[38;5;66;03m# MMR 기반 유사도 검색\u001b[39;00m\n\u001b[32m      4\u001b[39m     search_kwargs={\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10\u001b[39m,                    \u001b[38;5;66;03m# 전체 후보 문서 수\u001b[39;00m\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfetch_k\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m30\u001b[39m,              \u001b[38;5;66;03m# 더 많은 후보 중 다양성 고려해 k개 선택\u001b[39;00m\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlambda_mult\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.7\u001b[39m          \u001b[38;5;66;03m# 유사도 vs 다양성 균형 (1.0: 유사도만, 0.0: 다양성만)\u001b[39;00m\n\u001b[32m      8\u001b[39m     }\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "# MMR Retriever 적용\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",              # MMR 기반 유사도 검색\n",
    "    search_kwargs={\n",
    "        \"k\": 10,                    # 전체 후보 문서 수\n",
    "        \"fetch_k\": 30,              # 더 많은 후보 중 다양성 고려해 k개 선택\n",
    "        \"lambda_mult\": 0.7          # 유사도 vs 다양성 균형 (1.0: 유사도만, 0.0: 다양성만)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2b9463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Please answer the question in Korean\n",
    "\n",
    "    #Question:\n",
    "    {question}\n",
    "\n",
    "    #Context:\n",
    "    {context}\n",
    "\n",
    "    #Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(type(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4121757a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_23468\\3632315121.py:12: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# LLM 모델 로드 - ChatGPT\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
    "    model_name=\"gpt-3.5-turbo\",  # 모델명\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334b3492",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_parser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[32m      5\u001b[39m chain = (\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mretriever\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: RunnablePassthrough()}\n\u001b[32m      7\u001b[39m     | prompt\n\u001b[32m      8\u001b[39m     | llm\n\u001b[32m      9\u001b[39m     | StrOutputParser()\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# 단계 8 체인(Chain) 생성\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f3a1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI 지원사업의 지원자격은 AI컴퓨팅 실증 인프라 고도화 사업의 과업수행에 문제가 없는 기업 및 기관이 해당됩니다. 또한 국산 AI반도체 기반 AI컴퓨팅 인프라 구축, AI응용서비스 실증 수행, 클라우드 서비스 인프라 운영 및 제공, AI-IaaS 서비스 구축 및 운영 등의 역할을 수행할 수 있는 자격이 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 9 답변 생성\n",
    "question = 'AI 지원사업 지원자격 알려줘?'\n",
    "\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0f150",
   "metadata": {},
   "source": [
    "# ./data/ -> pdf 모든거 파일 임베딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 자동화\n",
    "import os\n",
    "import fitz\n",
    "import camelot\n",
    "from langchain.schema import Document\n",
    "from tabulate import tabulate\n",
    "\n",
    "# PymuPDF로 텍스트 분할\n",
    "def extract_text_with_fitz(pdf_path):\n",
    "    doc = fitz.open(pdf_path) # PDF 문서 열기\n",
    "    texts = []\n",
    "    \n",
    "    # PDF 문서 내 모든 페이지 순회\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num) # 페이지 단위로 로드 (0부터 시작)\n",
    "        text = page.get_text() # 텍스트 추출\n",
    "        texts.append(text.strip() if text else \"\") # 공백 제거 후 저장\n",
    "    return texts\n",
    "\n",
    "\n",
    "# camelot - lattice로 표 자르기\n",
    "def extract_tables_lattice_per_page(pdf_path):\n",
    "    \n",
    "    tables_per_page = {} # {페이지번호: [DataFrame, ...], ...} 형태 저장\n",
    "    \n",
    "    tables = camelot.read_pdf(pdf_path, pages=\"1-end\", flavor=\"lattice\") \n",
    "    # 모든 페이지 대상 표 추출\n",
    "    # (격자무늬 구조 표 추출에 적합)\n",
    "    \n",
    "    for table in tables:\n",
    "        page_num = table.page # Camelot이 인식한 페이지 번호(1부터 시작함)\n",
    "        \n",
    "        df = table.df.applymap(lambda x: x.replace('\\n', ' ') if isinstance(x, str) else x)\n",
    "        # isinstance(x, str): x가 문자열인지 검사\n",
    "        # 표 데이터를 DataFrame 형태로 얻음\n",
    "        # 셀 내 줄바꿈 문자를 공백으로 치환하여 한줄로 변환\n",
    "        \n",
    "        # 해당 페이지 번호가 딕셔너리에 없으면 새 리스트로 초기화\n",
    "        # ex) 3페이지에 첫 표가나오면, 3이라는 키가 없으니 생성하고 추가\n",
    "        if page_num not in tables_per_page:\n",
    "            tables_per_page[page_num] = []\n",
    "            \n",
    "        tables_per_page[page_num].append(df)\n",
    "        \n",
    "    return tables_per_page\n",
    "\n",
    "\n",
    "# 표 DataFrame 리스트를 마크다운 형식 표 문자열로 변환\n",
    "def convert_tables_to_markdown(tables):\n",
    "    \n",
    "    markdowns = []\n",
    "    \n",
    "    for i, df in enumerate(tables):\n",
    "        try:\n",
    "                \n",
    "            # 첫행일 헤더로 사용해 github 스타일 마크다운 표 생성\n",
    "            markdown_table = tabulate(df.values.tolist(), headers=df.iloc[0].tolist(), tablefmt=\"github\")\n",
    "            # df.values.tolist(): DataFrame 데이터를 2차원 리스트(리스트 안에 리스트)로 변환\n",
    "            # df.iloc[0].tolist(): DataFrame 첫 행을 리스트로 변환 -> 표 헤더로 사용\n",
    "            # tabulate(..., headers=..., tablefmt=\"github\"): 데이터를 마크다운 형식으로 예쁘게 변환\n",
    "            \n",
    "            markdowns.append(f\"▼ 표 {i + 1}:\\n{markdown_table}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            markdowns.append(f\"▼ 표 {i + 1}: 변환 실패 ({e})\")\n",
    "            \n",
    "    return \"\\n\\n\".join(markdowns)\n",
    "\n",
    "\n",
    "#모든 파일  자동화\n",
    "def load_all_pdfs_in_data_dir(data_dir=\"./data\"):\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    # ./data 폴더 내 모든 pdf 파일 리스트\n",
    "    pdf_files = [f for f in os.listdir(data_dir) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(data_dir, pdf_file)\n",
    "        file_name, _ = os.path.splitext(pdf_file)\n",
    "\n",
    "        texts = extract_text_with_fitz(pdf_path)\n",
    "        tables_per_page = extract_tables_lattice_per_page(pdf_path)\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            page_num = i + 1\n",
    "            tables = tables_per_page.get(page_num, [])\n",
    "            tables_str = convert_tables_to_markdown(tables)\n",
    "            combined_text = text\n",
    "            if tables_str:\n",
    "                combined_text += \"\\n\\n---\\n\\n📊 페이지 내 표 정보:\\n\" + tables_str\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=combined_text,\n",
    "                metadata={\n",
    "                    \"source_file\": file_name,\n",
    "                    \"page\": page_num\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_documents = load_all_pdfs_in_data_dir(\"./data\")\n",
    "\n",
    "    # 임베딩 및 벡터 DB 저장용으로 사용 가능\n",
    "    print(f\"총 문서 페이지 수: {len(all_documents)}\")\n",
    "    print(\"예시 문서 메타데이터:\", all_documents[0].metadata)\n",
    "    print(\"예시 문서 내용 일부:\", all_documents[0].page_content[:300])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
