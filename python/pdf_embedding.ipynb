{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# íŒŒì´ì¬ ê°€ìƒí™˜ê²½ ìƒì„±\n",
    "python -m venv .venv\n",
    "  \n",
    "(.venv) = ì„¤ì •í•˜ê³ ì ê°€ìƒí™˜ê²½ ì´ë¦„\n",
    "\n",
    "# ì£¼í”¼í„° ì»¤ë„ ë“±ë¡\n",
    "python -m ipykernel install --user --name=venv --display-name \"Python (.venv)\n",
    "(Python) = ì„¤ì •í•˜ê³ ì í•˜ëŠ” ê°€ìƒí™˜ê²½ ì´ë¦„\n",
    "\n",
    "\n",
    "PowerShellì—ì„œ ê°€ìƒí™˜ê²½ í™œì„±í™”\n",
    ".\\.venv\\Scripts\\Activate.PS1\n",
    "\n",
    "ê°€ìƒí™˜ê²½ ë¦¬ìŠ¤íŠ¸ í™•ì¸ ëª…ë ¹ì–´\n",
    "jupyter kernelspec list\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c96f0",
   "metadata": {},
   "source": [
    "### PyMuPDF + Camelot(Lattice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c609d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadError(\"Invalid Elementary Object starting with b')' @179086: b'ttp://smart.nipa.kr))\\\\n/S /URI >>\\\\n/H /I\\\\n/F 28\\\\n>>\\\\nendobj\\\\n311 0 obj\\\\n<< /Type /Annot'\")\n",
      "PdfReadError(\"Invalid Elementary Object starting with b')' @179278: b'(http://www.nipa.kr))\\\\n/S /URI >>\\\\n/H /I\\\\n/F 28\\\\n>>\\\\nendobj\\\\n309 0 obj\\\\n[\\\\n308 0 R\\\\n310 0'\")\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_23468\\2052424735.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = table.df.applymap(lambda x: x.replace('\\n', ' ') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import fitz\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "def extract_text_with_fitz(pdf_path):\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    texts = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        texts.append(text.strip() if text else \"\")\n",
    "        \n",
    "    return texts\n",
    "\n",
    "def extract_tables_lattice_per_page(pdf_path):\n",
    "    # í˜ì´ì§€ë³„ í‘œë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ {í˜ì´ì§€ë²ˆí˜¸(1-based): [DataFrame, ...], ...}\n",
    "    tables_per_page = {}\n",
    "\n",
    "    # Camelotìœ¼ë¡œ ëª¨ë“  í‘œë¥¼ ì¶”ì¶œ (í˜ì´ì§€ë³„ë¡œ ì¶”ì¶œì„ ëª»í•˜ë‹ˆ ê° í‘œì˜ í˜ì´ì§€ ì •ë³´ ì´ìš©)\n",
    "    tables = camelot.read_pdf(pdf_path, pages=\"1-end\", flavor=\"lattice\")\n",
    "    \n",
    "    for table in tables:\n",
    "        page_num = table.page  # Camelotì´ ì•Œë ¤ì£¼ëŠ” í˜ì´ì§€ ë²ˆí˜¸ (1-based)\n",
    "        df = table.df.applymap(lambda x: x.replace('\\n', ' ') if isinstance(x, str) else x)\n",
    "        if page_num not in tables_per_page:\n",
    "            tables_per_page[page_num] = []\n",
    "        tables_per_page[page_num].append(df)\n",
    "        \n",
    "\n",
    "    return tables_per_page\n",
    "\n",
    "def convert_tables_to_markdown(tables):\n",
    "    markdowns = []\n",
    "    for i, df in enumerate(tables):\n",
    "        try:\n",
    "            markdown_table = tabulate(df.values.tolist(), headers=df.iloc[0].tolist(), tablefmt=\"github\")\n",
    "            markdowns.append(f\"â–¼ í‘œ {i + 1}:\\n{markdown_table}\")\n",
    "        except Exception as e:\n",
    "            markdowns.append(f\"â–¼ í‘œ {i + 1}: ë³€í™˜ ì‹¤íŒ¨ ({e})\")\n",
    "    return \"\\n\\n\".join(markdowns)\n",
    "            \n",
    "\n",
    "# ê²½ë¡œ\n",
    "pdf_path = \"./data/2025ë…„ AIë°˜ë„ì²´ ì¡°ê¸° ìƒìš©í™” ë° AXì‹¤ì¦ ì§€ì› ì‚¬ì—… í†µí•© ê³µê³ ë¬¸.pdf\"\n",
    "\n",
    "# íŒŒì¼ëª…, í™•ì¥ì ë¶„ë¦¬\n",
    "file_with_ext = os.path.basename(pdf_path)\n",
    "file_name, file_ext = os.path.splitext(file_with_ext)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "texts = extract_text_with_fitz(pdf_path)\n",
    "\n",
    "# í˜ì´ì§€ë³„ í‘œ ì¶”ì¶œ\n",
    "tables_per_page= extract_tables_lattice_per_page(pdf_path)\n",
    "\n",
    "\n",
    "# í˜ì´ì§€ë³„ document ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "documents = []\n",
    "total_pages = len(texts)\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    page_num = i + 1\n",
    "    tables = tables_per_page.get(page_num, [])\n",
    "    \n",
    "    tables_Str = convert_tables_to_markdown(tables)\n",
    "    combined_text = text\n",
    "    \n",
    "    if tables_Str:\n",
    "        combined_text += \"\\n\\n---\\n\\nğŸ“Š í˜ì´ì§€ ë‚´ í‘œ ì •ë³´:\\n\" + tables_Str\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=combined_text, \n",
    "        metadata={\"page\": page_num,\n",
    "                  \"title\": file_name\n",
    "                  }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "# ì˜ˆì‹œ ì¶œë ¥\n",
    "with open(\"test/pdf.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(documents[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350ae6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 1, 'title': '2025ë…„ AIë°˜ë„ì²´ ì¡°ê¸° ìƒìš©í™” ë° AXì‹¤ì¦ ì§€ì› ì‚¬ì—… í†µí•© ê³µê³ ë¬¸'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í• \n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter()\n",
    "\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "split_documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99da2f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\KSVA\\Project\\python\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-nli', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹¨ê³„ 3: ì„ë² ë”© ìƒì„±\n",
    "\n",
    "# sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´  (ìœ„ì—ì„œ ì •ì˜í•¨)\n",
    "# HuggingFace ëª¨ë¸ì—ì„œ ì‚¬ìš©ëœ ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”© ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì„œ ì ìš©\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='jhgan/ko-sroberta-nli', # í•œêµ­ì–´ ìì—°ì–´ ì¶”ë¡  ìµœì¡±í™”ëœ  ko-sroberta ëª¨ë¸\n",
    "    model_kwargs={'device':'cpu'}, # CPUì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ì •\n",
    "    encode_kwargs={'normalize_embeddings':True}, # ì„ë² ë”©ì„ ì •ê·œí™”, ë²¡í„°ê°€ ê°™ì€ ë²”ìœ„ì˜ ê°’ì„ ê°–ë„ë¡ í•¨. (ìœ ì‚¬ë„ ê³„ì‚°ì‹œ ì¼ê´€ì„± ë†’ì„)\n",
    ")\n",
    "\n",
    "embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f50b407",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Chroma ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì €ì¥\u001b[39;00m\n\u001b[32m      5\u001b[39m vectorstore = Chroma.from_documents(\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     documents=\u001b[43msplit_documents\u001b[49m,\n\u001b[32m      7\u001b[39m     embedding=embeddings_model,\n\u001b[32m      8\u001b[39m     persist_directory=\u001b[33m\"\u001b[39m\u001b[33m./chroma_db\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m vectorstore\n",
      "\u001b[31mNameError\u001b[39m: name 'split_documents' is not defined"
     ]
    }
   ],
   "source": [
    "# ë‹¨ê³„ 4 : DB ìƒì„± ChromaDB ë° ì €ì¥\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Chroma ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\"  # ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥\n",
    ")\n",
    "\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0310e311",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriver) ìƒì„±\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m retriever = \u001b[43mvectorstore\u001b[49m.as_retriever()\n\u001b[32m      3\u001b[39m retriever\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "# ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriver) ìƒì„±\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9700ae6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# MMR Retriever ì ìš©\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m retriever = \u001b[43mvectorstore\u001b[49m.as_retriever(\n\u001b[32m      3\u001b[39m     search_type=\u001b[33m\"\u001b[39m\u001b[33mmmr\u001b[39m\u001b[33m\"\u001b[39m,              \u001b[38;5;66;03m# MMR ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰\u001b[39;00m\n\u001b[32m      4\u001b[39m     search_kwargs={\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10\u001b[39m,                    \u001b[38;5;66;03m# ì „ì²´ í›„ë³´ ë¬¸ì„œ ìˆ˜\u001b[39;00m\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfetch_k\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m30\u001b[39m,              \u001b[38;5;66;03m# ë” ë§ì€ í›„ë³´ ì¤‘ ë‹¤ì–‘ì„± ê³ ë ¤í•´ kê°œ ì„ íƒ\u001b[39;00m\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlambda_mult\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.7\u001b[39m          \u001b[38;5;66;03m# ìœ ì‚¬ë„ vs ë‹¤ì–‘ì„± ê· í˜• (1.0: ìœ ì‚¬ë„ë§Œ, 0.0: ë‹¤ì–‘ì„±ë§Œ)\u001b[39;00m\n\u001b[32m      8\u001b[39m     }\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "# MMR Retriever ì ìš©\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",              # MMR ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "    search_kwargs={\n",
    "        \"k\": 10,                    # ì „ì²´ í›„ë³´ ë¬¸ì„œ ìˆ˜\n",
    "        \"fetch_k\": 30,              # ë” ë§ì€ í›„ë³´ ì¤‘ ë‹¤ì–‘ì„± ê³ ë ¤í•´ kê°œ ì„ íƒ\n",
    "        \"lambda_mult\": 0.7          # ìœ ì‚¬ë„ vs ë‹¤ì–‘ì„± ê· í˜• (1.0: ìœ ì‚¬ë„ë§Œ, 0.0: ë‹¤ì–‘ì„±ë§Œ)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2b9463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Please answer the question in Korean\n",
    "\n",
    "    #Question:\n",
    "    {question}\n",
    "\n",
    "    #Context:\n",
    "    {context}\n",
    "\n",
    "    #Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(type(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4121757a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_23468\\3632315121.py:12: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# LLM ëª¨ë¸ ë¡œë“œ - ChatGPT\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ê°ì²´ ìƒì„±\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)\n",
    "    model_name=\"gpt-3.5-turbo\",  # ëª¨ë¸ëª…\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334b3492",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_parser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[32m      5\u001b[39m chain = (\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mretriever\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: RunnablePassthrough()}\n\u001b[32m      7\u001b[39m     | prompt\n\u001b[32m      8\u001b[39m     | llm\n\u001b[32m      9\u001b[39m     | StrOutputParser()\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# ë‹¨ê³„ 8 ì²´ì¸(Chain) ìƒì„±\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f3a1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI ì§€ì›ì‚¬ì—…ì˜ ì§€ì›ìê²©ì€ AIì»´í“¨íŒ… ì‹¤ì¦ ì¸í”„ë¼ ê³ ë„í™” ì‚¬ì—…ì˜ ê³¼ì—…ìˆ˜í–‰ì— ë¬¸ì œê°€ ì—†ëŠ” ê¸°ì—… ë° ê¸°ê´€ì´ í•´ë‹¹ë©ë‹ˆë‹¤. ë˜í•œ êµ­ì‚° AIë°˜ë„ì²´ ê¸°ë°˜ AIì»´í“¨íŒ… ì¸í”„ë¼ êµ¬ì¶•, AIì‘ìš©ì„œë¹„ìŠ¤ ì‹¤ì¦ ìˆ˜í–‰, í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì¸í”„ë¼ ìš´ì˜ ë° ì œê³µ, AI-IaaS ì„œë¹„ìŠ¤ êµ¬ì¶• ë° ìš´ì˜ ë“±ì˜ ì—­í• ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ìê²©ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ê³„ 9 ë‹µë³€ ìƒì„±\n",
    "question = 'AI ì§€ì›ì‚¬ì—… ì§€ì›ìê²© ì•Œë ¤ì¤˜?'\n",
    "\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0f150",
   "metadata": {},
   "source": [
    "# ./data/ -> pdf ëª¨ë“ ê±° íŒŒì¼ ì„ë² ë”©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ìë™í™”\n",
    "import os\n",
    "import fitz\n",
    "import camelot\n",
    "from langchain.schema import Document\n",
    "from tabulate import tabulate\n",
    "\n",
    "# PymuPDFë¡œ í…ìŠ¤íŠ¸ ë¶„í• \n",
    "def extract_text_with_fitz(pdf_path):\n",
    "    doc = fitz.open(pdf_path) # PDF ë¬¸ì„œ ì—´ê¸°\n",
    "    texts = []\n",
    "    \n",
    "    # PDF ë¬¸ì„œ ë‚´ ëª¨ë“  í˜ì´ì§€ ìˆœíšŒ\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num) # í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¡œë“œ (0ë¶€í„° ì‹œì‘)\n",
    "        text = page.get_text() # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        texts.append(text.strip() if text else \"\") # ê³µë°± ì œê±° í›„ ì €ì¥\n",
    "    return texts\n",
    "\n",
    "\n",
    "# camelot - latticeë¡œ í‘œ ìë¥´ê¸°\n",
    "def extract_tables_lattice_per_page(pdf_path):\n",
    "    \n",
    "    tables_per_page = {} # {í˜ì´ì§€ë²ˆí˜¸: [DataFrame, ...], ...} í˜•íƒœ ì €ì¥\n",
    "    \n",
    "    tables = camelot.read_pdf(pdf_path, pages=\"1-end\", flavor=\"lattice\") \n",
    "    # ëª¨ë“  í˜ì´ì§€ ëŒ€ìƒ í‘œ ì¶”ì¶œ\n",
    "    # (ê²©ìë¬´ëŠ¬ êµ¬ì¡° í‘œ ì¶”ì¶œì— ì í•©)\n",
    "    \n",
    "    for table in tables:\n",
    "        page_num = table.page # Camelotì´ ì¸ì‹í•œ í˜ì´ì§€ ë²ˆí˜¸(1ë¶€í„° ì‹œì‘í•¨)\n",
    "        \n",
    "        df = table.df.applymap(lambda x: x.replace('\\n', ' ') if isinstance(x, str) else x)\n",
    "        # isinstance(x, str): xê°€ ë¬¸ìì—´ì¸ì§€ ê²€ì‚¬\n",
    "        # í‘œ ë°ì´í„°ë¥¼ DataFrame í˜•íƒœë¡œ ì–»ìŒ\n",
    "        # ì…€ ë‚´ ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ í•œì¤„ë¡œ ë³€í™˜\n",
    "        \n",
    "        # í•´ë‹¹ í˜ì´ì§€ ë²ˆí˜¸ê°€ ë”•ì…”ë„ˆë¦¬ì— ì—†ìœ¼ë©´ ìƒˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”\n",
    "        # ex) 3í˜ì´ì§€ì— ì²« í‘œê°€ë‚˜ì˜¤ë©´, 3ì´ë¼ëŠ” í‚¤ê°€ ì—†ìœ¼ë‹ˆ ìƒì„±í•˜ê³  ì¶”ê°€\n",
    "        if page_num not in tables_per_page:\n",
    "            tables_per_page[page_num] = []\n",
    "            \n",
    "        tables_per_page[page_num].append(df)\n",
    "        \n",
    "    return tables_per_page\n",
    "\n",
    "\n",
    "# í‘œ DataFrame ë¦¬ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ í‘œ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "def convert_tables_to_markdown(tables):\n",
    "    \n",
    "    markdowns = []\n",
    "    \n",
    "    for i, df in enumerate(tables):\n",
    "        try:\n",
    "                \n",
    "            # ì²«í–‰ì¼ í—¤ë”ë¡œ ì‚¬ìš©í•´ github ìŠ¤íƒ€ì¼ ë§ˆí¬ë‹¤ìš´ í‘œ ìƒì„±\n",
    "            markdown_table = tabulate(df.values.tolist(), headers=df.iloc[0].tolist(), tablefmt=\"github\")\n",
    "            # df.values.tolist(): DataFrame ë°ì´í„°ë¥¼ 2ì°¨ì› ë¦¬ìŠ¤íŠ¸(ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë¦¬ìŠ¤íŠ¸)ë¡œ ë³€í™˜\n",
    "            # df.iloc[0].tolist(): DataFrame ì²« í–‰ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ -> í‘œ í—¤ë”ë¡œ ì‚¬ìš©\n",
    "            # tabulate(..., headers=..., tablefmt=\"github\"): ë°ì´í„°ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì˜ˆì˜ê²Œ ë³€í™˜\n",
    "            \n",
    "            markdowns.append(f\"â–¼ í‘œ {i + 1}:\\n{markdown_table}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            markdowns.append(f\"â–¼ í‘œ {i + 1}: ë³€í™˜ ì‹¤íŒ¨ ({e})\")\n",
    "            \n",
    "    return \"\\n\\n\".join(markdowns)\n",
    "\n",
    "\n",
    "#ëª¨ë“  íŒŒì¼  ìë™í™”\n",
    "def load_all_pdfs_in_data_dir(data_dir=\"./data\"):\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    # ./data í´ë” ë‚´ ëª¨ë“  pdf íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "    pdf_files = [f for f in os.listdir(data_dir) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(data_dir, pdf_file)\n",
    "        file_name, _ = os.path.splitext(pdf_file)\n",
    "\n",
    "        texts = extract_text_with_fitz(pdf_path)\n",
    "        tables_per_page = extract_tables_lattice_per_page(pdf_path)\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            page_num = i + 1\n",
    "            tables = tables_per_page.get(page_num, [])\n",
    "            tables_str = convert_tables_to_markdown(tables)\n",
    "            combined_text = text\n",
    "            if tables_str:\n",
    "                combined_text += \"\\n\\n---\\n\\nğŸ“Š í˜ì´ì§€ ë‚´ í‘œ ì •ë³´:\\n\" + tables_str\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=combined_text,\n",
    "                metadata={\n",
    "                    \"source_file\": file_name,\n",
    "                    \"page\": page_num\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_documents = load_all_pdfs_in_data_dir(\"./data\")\n",
    "\n",
    "    # ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ìš©ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥\n",
    "    print(f\"ì´ ë¬¸ì„œ í˜ì´ì§€ ìˆ˜: {len(all_documents)}\")\n",
    "    print(\"ì˜ˆì‹œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\", all_documents[0].metadata)\n",
    "    print(\"ì˜ˆì‹œ ë¬¸ì„œ ë‚´ìš© ì¼ë¶€:\", all_documents[0].page_content[:300])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
